


# cell for imports
import os
from doctest import testmod
from typing import NamedTuple

import numpy as np
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.session import SparkSession





spark: SparkSession = SparkSession.builder.getOrCreate()





spark.sparkContext.setLogLevel("Error")








class Item(NamedTuple):
    name: str
    quantity: tuple[int, str]
    price: float


groceries: list[Item] = [
    Item(name="courgette", quantity=(1, "piece"), price=0.75),
    Item(name="lentiles", quantity=(150, "grams"), price=1.45),
    Item(name="desert", quantity=(2, "piece"), price=4.0),
]





repr(groceries[0])





total: float = sum([item.price for item in groceries])
total





groceries_shoddy: list[tuple[str, int, float]] = [
    ("courgette", 1, 0.75),
    ("lentils", 1, 1.45),
    ("desert", 2, 4.0),
]
total: float = sum([item[2] for item in groceries_shoddy])
total





groceries_shoddy: list[tuple[str, tuple[int, str], float]] = [
    ("courgette", (1, "piece"), 0.75),
    ("lentils", (150, "gram"), 1.45),
    ("desert", (2, "piece"), 4.0),
]
type(groceries_shoddy[1][1])


for t in groceries:
    print(type(t.quantity))





df_groceries: DataFrame = spark.createDataFrame(
    groceries, ["item", "quantity", "price"]
)
df_groceries





df_groceries.printSchema()





path: str = (
    "./ProgrammingProjects/SparkTest/DataAnalysisWithPythonAndPySpark-Data-trunk/gutenberg_books/"
)

pride_and_pred: DataFrame = spark.read.text(path + "1342-0.txt")





pride_and_pred.printSchema()


pride_and_pred.show(truncate=False)


f"the row count of pride_and pred is {pride_and_pred.count()}"


pride_and_pred.rdd.countApprox(timeout=5)


pride_and_pred.columns


pride_and_pred.dtypes





lines: DataFrame = pride_and_pred.select(
    F.split(pride_and_pred.value, " ").alias("line")
)

lines.show(n=5, truncate=False)





words: DataFrame = lines.select(F.explode(F.col("line")).alias("word"))
words.show()





words_lower: DataFrame = words.select(F.lower(F.col("word")).alias("word_lower"))
words_lower.show()


words_clean: DataFrame = words_lower.select(
    F.regexp_extract(F.col("word_lower"), "[a-z]+", 0).alias("word")
)
words_clean.show()


words_no_null: DataFrame = words_clean.filter(F.col("word") != "")
words_no_null.show()





big_words = words_no_null.filter(F.length(F.col("word")) > 11)
big_words.count()





path: str = "./Downloads"

broadcast_logs: DataFrame = spark.read.csv(
    path=os.path.join(path, "BroadcastLogs_2018_Q3_M8.CSV"),
    sep="|",
    header=True,
    inferSchema=True,
    timestampFormat="yyyy-MM-dd",
)





broadcast_logs.select("BroadcastLogID", "LogServiceID", "LogDate").show(
    n=5, truncate=False
)





broadcast_logs.select(
    F.col("Duration"),
    F.col("Duration").substr(1, 2).cast("int").alias("hours"),
    F.col("Duration").substr(4, 2).cast("int").alias("Minutes"),
    F.col("Duration").substr(7, 2).cast("int").alias("Seconds"),
).distinct().show(5)





broadcast_logs.select(
    F.col("Duration"),
    (
        F.col("Duration").substr(1, 2).cast("int") * 3600
        + F.col("Duration").substr(4, 2).cast("int") * 60
        + F.col("Duration").substr(7, 2).cast("int")
    ).alias("DurationInSeconds"),
).distinct().show(5)


broadcast_logs.printSchema()


len(broadcast_logs.columns)





broadcast_logs = broadcast_logs.select(
    *[x for x in broadcast_logs.columns if x[-2:] != "ID"]
)
broadcast_logs.printSchema()





extended_logs = broadcast_logs.withColumn(
    "DurationInSeconds",
    (
        F.col("Duration").substr(1, 2).cast("int") * 3600
        + F.col("Duration").substr(4, 2).cast("int") * 60
        + F.col("Duration").substr(7, 2).cast("int")
    ),
)
extended_logs.printSchema()





extended_logs.select(sorted(extended_logs.columns)).printSchema()





extended_logs.select(F.col("DurationInSeconds")).describe().show()


extended_logs.select(F.col("DurationInSeconds")).summary().show()






