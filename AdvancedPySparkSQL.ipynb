{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02fb122f-b702-45a6-b9c2-650c1f7e237b",
   "metadata": {},
   "source": [
    "## Advanced PySpark SQL operations with Python\n",
    "This notebook covers advanced PySpark, SQL, and Python features. Topics this notebook covers are:\n",
    "\n",
    "1. Functional-style programming in Python. Though Python is not a functional language, it can be used in a functional style. This style is prevalent in data science and engineering. Also, a good understanding of the map-reduce manner of programming will help understand the RDD, the core PySpark data-structure the DataFrame is built upon.\n",
    "2. Blending SQL with PySpark Python methods. There are a few PySpark Python functions that accept SQL expressions.\n",
    "3. Caching in PySpark, or how to improve the speed at which we access frequently requested data.\n",
    "4. Windows functions, also known as analytical functions, allow you to use the value from one or more rows to return a value for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d350e76-29af-4bf0-950b-b99cb047cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell for imports\n",
    "from functools import lru_cache, reduce\n",
    "from itertools import takewhile\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession, Window, WindowSpec\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c87d7-06e9-44a1-8325-35af5b138430",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark: SparkSession = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5520c-5a53-468d-ae57-8e8b2cfb66d8",
   "metadata": {},
   "source": [
    "#### Revisting the Backblaze data using Functional Python\n",
    "Unlike Scala, the language Spark is written in, Python is not a functional programming language. However it has two excellent functional modules: [functools](https://docs.python.org/3/library/functools.html) and [itertools](https://docs.python.org/3/library/itertools.html) which contain functions inspired by functional languages like Haskell and the ML family of programming languages. These functions, like reduce or accumulate, used in conjunction with lambda expressions, are very common in Python data engineering and science because their use is very succinct.\n",
    "\n",
    "If you have a particular problem, for instance, you want to take from a collection all the elements until some result is met. You could look at takewhile in the itertools library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d22d4-ff7b-42e4-9804-bd2ccd485f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(takewhile(lambda x: x < 4, [1, 2, 4, 3, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962889d4-a5a2-4903-b682-f117ef59a9c1",
   "metadata": {},
   "source": [
    "Of course, this is a quite meaningless example. I could easily make it a more useful example. something with streams of data that you want to take while some stop signal is not met.\n",
    "\n",
    "A perfect example of functional-style programming is using reduce and lambda expressions to redesign the steps we took to make the Backblaze data usable; see the PySparkSQL notebook for the original steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551f2be-1d2d-4c36-b744-394d51d6ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILES: list[str] = [\n",
    "    \"drive_stats_Q1\",\n",
    "    \"drive_stats_Q2\",\n",
    "    \"drive_stats_Q3\",\n",
    "    \"drive_stats_Q4\",\n",
    "]\n",
    "\n",
    "data: list[DataFrame] = [\n",
    "    spark.read.csv(file, header=True, inferSchema=True) for file in DATA_FILES\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700eeae6-acb3-4def-b064-2ade9161c86c",
   "metadata": {},
   "source": [
    "Previously, we checked the length of the columns and added the extra columns of Q4 to the others. \n",
    "This is an option, but inspecting our CSV files shows us that added information is really of little interest to our investigation, so let us just keep the common columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed41279-ee02-4dbf-b947-2c176236cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns: set[str] = reduce(\n",
    "    lambda x, y: x.intersection(y), [set(df.columns) for df in data]\n",
    ")\n",
    "# fail fast\n",
    "try:\n",
    "    assert set([\"model\", \"failure\", \"date\", \"capacity_bytes\"]).issubset(common_columns)\n",
    "except AssertionError:\n",
    "    print(\"Not all the columns are the same\")\n",
    "\n",
    "full_data: DataFrame = reduce(\n",
    "    lambda x, y: x.select(list(common_columns)).union(y.select(list(common_columns))),\n",
    "    data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6bf59-f82e-4700-93fe-a820406003c5",
   "metadata": {},
   "source": [
    "#### Analysing the code\n",
    "I packed the cell with three actions. The first action is to create a set of column names that the four quarters have in common. I used a $\\lambda$ function, a reduce function, a list comprehension, and a few Python functions to achieve the result. \n",
    "\n",
    "- First I created a set of columns for each DataFrame in data: `[set(df.columns) for df in data]`. It is important that I cast the list `df.columns` because I want to use the `intersection` method of the `set` class.\n",
    "- Than I feed two sets of columns as arguments to the lambda function: `x.intersection(y)`. Basic set theory teaches us that this will return only those columns that both sets have in common. A lambda, or $\\lambda$ function, is an anonimous function; it has no name and is not stored separately in memory. It lives as long as the encompassing function lives.\n",
    "- The reduce function is a programming construct that recursively walks through an iterable and applies a function to the elements, finally putting the result back together. This type of function is more commonly known as a `Fold`. The final result of a fold can be put back together in two ways:\n",
    "  \n",
    "1. From right to left -> `reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates (1+(2+(3+(4+5))))` The outermost `(4+5)` is evaluated first. This is the way Haskell `foldr` is evaluated. This type of evaluation leaves a lot of function calls on the memory stack.\n",
    "2. Python builds it back together from left to right -> `reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5)`. The innermost addition `(1+2)` is evaluated first, this has a smaller memory stack but requires the use of intermediary results, which need to be cached.\n",
    "\n",
    "In Python you should only use reduce with an operation that has the [associative property](https://en.wikipedia.org/wiki/Associative_property), as $(1-(2-3))\\ne((1-2)-3)$. PySparks reduce is only defined for associative operations. In Scala or Haaskel you would use left or right fold as the operation would dictate.   \n",
    "\n",
    "The second action is a fail fast. Before I do the computationally expensive third step, I make sure that the result is as I expected. If the third action failed, I would have to debug the code, and I might have to trace a long log to find the fault. This way, it is called localize failures, I avoid having to do that. In general you want to fail fast with:\n",
    "1. Network calls\n",
    "2. Database start up\n",
    "3. Securing RESTful APIs with request validation, usually a token of sorts.\n",
    "\n",
    "More information on fail fast can be found in this article by [Martin Fowler](https://martinfowler.com/ieeeSoftware/failFast.pdf)\n",
    "\n",
    "The third action does the heavy lifting by performing a union operation on two selects. Again, I use the `reduce` and the $\\lambda$ to keep the code terse.\n",
    "\n",
    "This kind of code, where functional programming concepts such as `comprehensions`, `reduce`, and `lambda` do the heavy lifting, is very common in data science and engineering. You need to be able to read this kind of code. I also use this style of coding when talking about RDDs and UDFs in my notebook about that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818da656-4f67-41f2-9071-39002fc39cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.select([\"model\", \"failure\", \"date\", \"capacity_bytes\"]).where(\n",
    "    \"failure == 1\"\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246e645-820f-42d0-b708-f6116f36fbc3",
   "metadata": {},
   "source": [
    "#### SQL expressions in PySpark\n",
    "I guess that the above code and analysis of that code were difficult enough, so on a lighter note: There are three PySpark methods that accept SQL expressions:\n",
    "1. `selectExpr`: Similar to the `select` statement, but just excepts SQL expressions.\n",
    "2. `expr()`: allows you to use an SQL expression within a standard select\n",
    "3. `filter`/ `where` accept SQL expressions as well if given as a string\n",
    "\n",
    "You can use these methods to blend Python code with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2980d-7686-4fa2-b099-7b56bd720611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([[\"George\"], [\"Rhino\"]], [\"name\"])\n",
    "df.select(\"name\", F.expr(\"length(name)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032944b9-c864-4324-bce3-219f27f55f6f",
   "metadata": {},
   "source": [
    "consider the following select statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ea012-60ba-4fc2-87c0-0da1e60269d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_select = full_data.withColumn(\n",
    "    \"capacity_in_GB\", F.round(F.col(\"capacity_bytes\") / pow(1024, 3), 0)\n",
    ").select(\"model\", \"failure\", \"date\", \"capacity_in_GB\")\n",
    "\n",
    "full_data_select.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c28ab-31b1-4c81-b26b-1f2a117be3ca",
   "metadata": {},
   "source": [
    "Using the select expression we write this more terse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df314169-fcb8-4d6d-a496-e6b86e532fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_gb: DataFrame = full_data.selectExpr(\n",
    "    \"model\",\n",
    "    \"failure\",\n",
    "    \"date\",\n",
    "    \"round(capacity_bytes / pow(1024,3),0) as capacity_in_GB\",\n",
    ")\n",
    "\n",
    "full_data_gb.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cefcdce-c93d-436b-8c15-abeafc763576",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_days = full_data_gb.groupBy(\"model\", \"capacity_in_GB\").agg(\n",
    "    F.count(\"*\").alias(\"drive_days\")\n",
    ")\n",
    "\n",
    "failures = (\n",
    "    full_data_gb.filter(\"failure == 1\")\n",
    "    .groupBy(\"model\", \"capacity_in_GB\")\n",
    "    .agg(F.count(\"*\").alias(\"failures\"))\n",
    ")\n",
    "\n",
    "summarized_data = (\n",
    "    drive_days.join(other=failures, on=[\"model\", \"capacity_in_GB\"], how=\"left\")\n",
    "    .fillna(value=0.0, subset=[\"failures\"])\n",
    "    .selectExpr(\"model\", \"capacity_in_GB\", \"failures / drive_days as failure_rate\")\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "summarized_data.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c543418a-0b96-424d-a6ba-7ff93059294f",
   "metadata": {},
   "source": [
    "What can you do with all this information? You can write a simple function to present the most\n",
    "reliable hard disk drive for its capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f7bac-1a24-4b67-8387-67778746e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliable_drive_for_capacity(\n",
    "    data: DataFrame, capacity_in_GB: int = 2048, precision: float = 0.25, top_n: int = 3\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    desc: Function that returns the top N drives with lowest failure rate for the given capacity\n",
    "    test: No tests as I need to mock a DataFrame.\n",
    "    \"\"\"\n",
    "    capacity_min = capacity_in_GB / (1 + precision)\n",
    "    capacity_max = capacity_in_GB * (1 + precision)\n",
    "    return (\n",
    "        data.filter(f\"capacity_in_gb between {capacity_min} and {capacity_max}\")\n",
    "        .orderBy(\"failure_rate\", \"capacity_in_GB\", ascending=[True, False])\n",
    "        .limit(top_n)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7034f-1d90-47b7-8469-0163f8449344",
   "metadata": {},
   "outputs": [],
   "source": [
    "reliable_drive_for_capacity(data=summarized_data, capacity_in_GB=11176).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fea4ec-f563-42cc-aae7-f88db5443e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reliable_drive_for_capacity(data=summarized_data, capacity_in_GB=6500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0963de7-652e-4f7a-b73e-60b393ca0e10",
   "metadata": {},
   "source": [
    "You can perform this operation without a join. This solution requires quite some insight into PySpark, \n",
    "I would not have come up with this solution. I also wonder if it is the clearest solution for the reader. It involves kind of a trick. 1/0 are equivalent in Python to True/False (they are \"truthy\" values). We can use sum as both filter (when we sum the 0's won't be counted, we are thus filtering on the ones) and counting clauses. The reason to use this kind of code without a join, is that it is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe616e-5463-4cae-8efe-4cf55e046ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinless: DataFrame = (\n",
    "    full_data_gb.groupBy(\"model\", \"capacity_in_GB\")\n",
    "    .agg(F.sum(\"failure\").alias(\"failures\"), F.count(\"*\").alias(\"drive_days\"))\n",
    "    .selectExpr(\"model\", \"capacity_in_GB\", \"failures / drive_days as failure_rate\")\n",
    "    .cache()\n",
    ")\n",
    "joinless.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3bb44-ec4f-482b-8a03-0f055729dac2",
   "metadata": {},
   "source": [
    "What does the `cache` function do? If you look up the PySpark documentation on [cache](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.cache.html?highlight=cache#pyspark.sql.DataFrame.cache), you get a perfectly nothing-saying answer: Persists the DataFrame with the default storage level. It really should be saying `cache` stores intermediary DataFrames for later computational use. Meaning that instead of recomputing all results, caching stores some results in memory so later calculations can use these partial results. \n",
    "\n",
    "Why is caching so important? Three reasons:\n",
    "\n",
    "1. Cost-efficiency. Distributed computations, like PySpark's, are expensive. Reusing computations saves money, especially in environments like Azure or GCC where you may have to pay for CPU usage.\n",
    "2. Time-efficiency. Obviously, you save time by not having to redo computations but by using stored results.\n",
    "3. Exexcution-efficiency: Again, quite obviously, not having to redo computation means the execution will be more efficient and you can perform more jobs per cluster.\n",
    "\n",
    "The easiest way to see the advantages of caching results is with an example. Take a function that tell you the n-th Fibonacci number: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964cccc-ed9c-46a7-a750-5933e23ebb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n: int) -> int:\n",
    "    return n if n < 2 else fib(n - 1) + fib(n - 2)\n",
    "\n",
    "\n",
    "%time fib(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf8170-e4a6-40c1-9e55-d0991cd1008d",
   "metadata": {},
   "source": [
    "As you will see if you run the cell below, the time difference is enormous. We will move from a runtime of seconds to one of microseconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f19714-59cc-4b57-9503-034472d6dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lru = least recently used\n",
    "@lru_cache\n",
    "def fib(n: int) -> int:\n",
    "    return n if n < 2 else fib(n - 1) + fib(n - 2)\n",
    "\n",
    "\n",
    "%time fib(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7397c7c-b7ee-4a81-9aa2-50f115c01c0a",
   "metadata": {},
   "source": [
    "## Window functions\n",
    "Coming from SQL a window function, also known as an analytical function, is a function that uses the value from one or more rows to return a value for each row; the rows keep their identity. This is in contrast to the aggregate function, which returns a single value for multiple rows. \n",
    "\n",
    "From the [PostgresSQL](https://www.postgresql.org/docs/current/tutorial-window.html) documentation comes the following example:\n",
    "\n",
    "`SELECT depname, empno, salary, avg(salary) OVER (PARTITION BY depname) FROM empsalary;`\n",
    "\n",
    "Windows functions have an `over` clause and a `partion by` clause. The `partion by` divides the rows into mutually exclusive sets of rows. In the above example, the sets are made on `depname`. Over those subsets, we apply the agg function `avg`. In essence a windows function is no different than regular SQL function, but instead of applying to all the rows, we apply them to subsets of the rows. Resulting in the following combination:\n",
    "\n",
    "  depname  | empno | salary |          avg\n",
    "-----------|-------|--------|-----------------------\n",
    " develop   |    11 |   5200 | 5020.0000000000000000\n",
    " develop   |     7 |   4200 | 5020.0000000000000000\n",
    " develop   |     9 |   4500 | 5020.0000000000000000\n",
    " develop   |     8 |   6000 | 5020.0000000000000000\n",
    " develop   |    10 |   5200 | 5020.0000000000000000\n",
    " personnel |     5 |   3500 | 3700.0000000000000000\n",
    " personnel |     2 |   3900 | 3700.0000000000000000\n",
    " sales     |     3 |   4800 | 4866.6666666666666667\n",
    " sales     |     1 |   5000 | 4866.6666666666666667\n",
    " sales     |     4 |   4800 | 4866.6666666666666667\n",
    "\n",
    "\n",
    "A window function follows the same split-apply-combine pattern we have seen before. In PySpark, we need to have a WindowSpec object that specifies how we partition our DataFrame into N mutually exclusive sets of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea36f5-5960-497b-83e8-d7d7974240df",
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = (\n",
    "    \"./ProgrammingProjects/SparkTest/DataAnalysisWithPythonAndPySpark-Data-trunk/\"\n",
    ")\n",
    "\n",
    "gsod: DataFrame = (\n",
    "    reduce(\n",
    "        lambda x, y: x.unionByName(y, allowMissingColumns=True),\n",
    "        [\n",
    "            spark.read.parquet(f\"{path}gsod_noaa/gsod{year}.parquet\")\n",
    "            for year in range(2010, 2021)\n",
    "        ],\n",
    "    )\n",
    "    .dropna(subset=[\"year\", \"mo\", \"da\", \"temp\"])\n",
    "    .where(F.col(\"temp\") != 9999.9)\n",
    "    .drop(\"date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c8802-5b82-4788-8f02-b9ecbff19aa6",
   "metadata": {},
   "source": [
    "I am using a Pandas user defined function (UDF) here to bring the results to a European scale that we understand more intuitively. I discuss user defined functions in the RDD, UDF, and Pandas notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce354b7-9cd7-475a-9ae3-3c53d7d92d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(\"double\")\n",
    "def fahrenheit_to_celsius(degrees: pd.Series) -> pd.Series:\n",
    "    \"\"\"converts degrees in Fahrenheit to Celsius\"\"\"\n",
    "    return round((degrees - 32) * 5 / 9, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f9c726-e70c-4ae7-8b2f-1d99192bb943",
   "metadata": {},
   "source": [
    "#### summarize-join-approach\n",
    "\n",
    "What we can do with window functions, create mutually exclusive sets of rows, apply a function to \n",
    "these rows and combine them back together, we can do without a window function. See the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03f2ee-13d4-4cdb-ae3c-2118ec8f20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "coldest_temp: DataFrame = gsod.groupBy(\"year\").agg(\n",
    "    F.min(fahrenheit_to_celsius(\"temp\")).alias(\"temp\")\n",
    ")\n",
    "coldest_temp.orderBy(\"temp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da3ea0-35e5-452c-b40c-556413d74de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coldest_when: DataFrame = gsod.join(\n",
    "    other=coldest_temp, on=[\"year\", \"temp\"], how=\"left_semi\"\n",
    ").select(\"stn\", \"year\", \"mo\", \"da\", \"temp\")\n",
    "coldest_when.orderBy(\"year\", \"mo\", \"da\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228be6c7-379c-43ea-a17d-3214318713fa",
   "metadata": {},
   "source": [
    "#### self-join\n",
    "This join is a bit odd; we first create a subset of our GSOD data coldest_temp and then join it with GSOD (which contains the subset) again. This type of self-join is considered an anti-pattern (bad programming practice) in data manipulation. Though obviously technically possible and not per se wrong, it is wieldy, slow, and feels not right.\n",
    "\n",
    "You can solve this better using a window function; this will require less code and will be faster, first step is to get `WindowSpec` object, with which we can partition our DataFrame into mutually exclusive sets of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a453e-38d2-4a87-accf-23f4302c7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "each_year: WindowSpec = Window.partitionBy(\"year\")\n",
    "each_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57871cc6-6283-4bca-bf51-50f0b61db767",
   "metadata": {},
   "source": [
    "#### split-apply-combine revisited\n",
    "We first need to create a specification for a windows function.\n",
    "\n",
    "1. Split: The `WindowSpec` object, will tell the window function to spit the DataFrame in mutually exclusive sets of rows according to the values in the year column.\n",
    "2. Apply: We apply the agg `min` function [`over`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.over.html?highlight=over#pyspark.sql.Column.over) our column.\n",
    "3. Combine: the combining function is implicit. You do not need to code that part, if you observe the feedback Spark gives you will observe the combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5d74a-0fcf-403d-87c7-2de85fadaf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = (\n",
    "    gsod.withColumn(\n",
    "        colName=\"min_temp\", col=F.min(fahrenheit_to_celsius(\"temp\")).over(each_year)\n",
    "    )\n",
    "    .where(\"temp = min_temp\")\n",
    "    .select(\"stn\", \"year\", \"mo\", \"da\", \"temp\")\n",
    "    .orderBy(\"year\", \"mo\", \"da\")\n",
    ")\n",
    "wf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264b165-fe1a-4009-8572-2b5256a25d6c",
   "metadata": {},
   "source": [
    "#### Mutually exclusive sets\n",
    "A DataFrame has partitions. In PySpark, these partitions are the physical splits of the data on each executor node. Partitioning itself means no more than a segmentation of, in our case, rows into mutually exclusive sets of rows. I do not think the vocabulary should be confusing; after all, the DataFrame partitioning means the same. Creating sets of rows that are mutually exclusive. The exclusivity being the executor nodes the set runs on. However, sometimes it is clarifying to have more names for the same idea: partion = mutually exclusive set.\n",
    "\n",
    "We can create our mutually exclusive sets based on more than one attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11432e-be05-4357-8173-466385fe1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "hottest_day: WindowSpec = Window.partitionBy(\"year\", \"mo\", \"da\")\n",
    "\n",
    "(\n",
    "    gsod.select(\n",
    "        \"stn\",\n",
    "        \"year\",\n",
    "        \"mo\",\n",
    "        \"da\",\n",
    "        \"temp\",\n",
    "        F.max(fahrenheit_to_celsius(\"temp\")).over(hottest_day).alias(\"max_temp\"),\n",
    "    )\n",
    "    .filter(F.col(\"temp\") == F.col(\"max_temp\"))\n",
    "    .drop(\"temp\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea9e375-bdd6-40bb-8d9d-0ac387b29bcd",
   "metadata": {},
   "source": [
    "## Ranking functions\n",
    "Ranking functions do exactly what you would expect them to do: rank records based upon some value of a field. There are several distinctive ranking functions:\n",
    "\n",
    "1. `rank`: non-consecutive ranks\n",
    "2. `dense_rank`: consecutive ranks\n",
    "3. `percent_rank`: percentille ranks\n",
    "4. `ntile`: tiles\n",
    "5. `row_number`: return the row number\n",
    "\n",
    "To be able to rank we need to order by, which we can do in SQL like so:\n",
    "\n",
    "```\n",
    "SELECT depname, empno, salary,\n",
    "       rank() OVER (PARTITION BY depname ORDER BY salary DESC)\n",
    "FROM empsalary;\n",
    "```\n",
    "In PySpark we need the WindowSpec object to do the splitting, the same object can also do the ordering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0abf7-48cf-45df-bf65-abb9a1713bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_per_month_asc: WindowSpec = Window.partitionBy(\"mo\").orderBy(\"count_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667681c-75cb-4132-ac22-7dc1181234e9",
   "metadata": {},
   "source": [
    "To keep things running on a single PC the book provides a smaller GSOD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ec30c-c78d-4d4c-8ac6-c9b550ee80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = (\n",
    "    \"./ProgrammingProjects/SparkTest/DataAnalysisWithPythonAndPySpark-Data-trunk/window/\"\n",
    ")\n",
    "\n",
    "gsod_light: DataFrame = spark.read.parquet(path + \"gsod_light.parquet\")\n",
    "gsod_light.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec61784-842e-453e-b616-0260e63613f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod_light.withColumn(\"rank_tmp\", F.rank().over(temp_per_month_asc)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6334dd-3329-4d74-8db4-e6a57a775da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_each_year: WindowSpec = each_year.orderBy(\"temp\")\n",
    "gsod_light.withColumn(\"rank_tmp\", F.percent_rank().over(temp_each_year)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415c077-5780-416e-ad2a-f5de826c7ac7",
   "metadata": {},
   "source": [
    "The above result needs some explanation. If we take the second to last example. This record has two \n",
    "records in 2019 with a value < 44.7 with a total of four records, in the partion. The formula for calculating the percentile rank is:\n",
    "\n",
    "$\\frac{\\text{records with a lower value than the current one}}{\\text{number of records in the window - 1}}$\n",
    "\n",
    "or \n",
    "\n",
    "$\\frac{2}{4-1} = 0.667$\n",
    "\n",
    "Percentile rank is useful if you want to reflect where a value stands in comparisson to its peers in the window. \n",
    "\n",
    "We can ask `row_number` if we want to know where the records sits in the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806b3ea-bc55-46b2-a2a2-eb639e932f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod_light.withColumn(\"rank_tmp\", F.row_number().over(temp_each_year)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7deeae-69b6-43df-98ed-690b8c07fcc8",
   "metadata": {},
   "source": [
    "There is no ascending parameter in the orderBy method on a Window, unlike the orderBy method of a \n",
    "DataFrame. See the [documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.orderBy.html#pyspark.sql.Window.orderBy). If you want a reversed order you will have to include that in your specification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88816e90-d7cb-4c8e-9a3a-1f3de463144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_per_month_desc: WindowSpec = Window.partitionBy(\"mo\").orderBy(\n",
    "    F.col(\"count_temp\").desc()\n",
    ")\n",
    "\n",
    "gsod_light.withColumn(\"rank_tmp\", F.row_number().over(temp_per_month_desc)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313d2dc-4dcb-477e-a36f-d7222254c2c8",
   "metadata": {},
   "source": [
    "## analytic functions\n",
    "Of course there more analytical functions that the rank functions. Analytic functions calculate an aggregate value based on a set of rows. Unlike aggregate functions, analytic functions can return multiple rows for each window. We use analytic functions to compute moving averages, running totals, percentages or top-N results within a group.\n",
    "\n",
    "Examples of analytical functions are:\n",
    "\n",
    "- `lag`: Accesses data from a previous row in the same result set\n",
    "- `lead`: Accesses data from a subsequent row in the same result set \n",
    "- `cume_dist`: Calculates the relative position of a specified value in a group of values. Assuming ascending ordering, the cumelative distance of a value in row r is defined as: $\\frac{\\text{the number of rows with values less than or equal to that value in row r}}{\\text{the number of rows evaluated in the partition}}$\n",
    "\n",
    "As we can see from the formula, cummulative distance is closely related to rank precentile. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269cac30-9211-4190-b0db-e8bea3e96419",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod_light.withColumn(\"previous_temp\", F.lag(\"temp\").over(temp_each_year)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6140816-d810-4d28-a5fd-6c6c6b5f63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod_light.withColumn(\"rank_tmp\", F.percent_rank().over(temp_each_year)).withColumn(\n",
    "    \"cumulative_distance\", F.cume_dist().over(temp_each_year)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b8d72-624e-4e8d-963d-f5fdd89615db",
   "metadata": {},
   "source": [
    "#### Window Frames\n",
    "Consider the code below, we use two average function one on an unordered window, one on ordered window. We get a different result, which is unexpected as ordering should not make a difference on `avg`. Yet, looking at the result columns we see there is a problem using the ordered window.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84018621-cd81-4060-9c60-555eee391c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_ordered: WindowSpec = Window.partitionBy(\"year\")\n",
    "ordered: WindowSpec = not_ordered.orderBy(\"temp\")\n",
    "\n",
    "gsod_light.withColumn(\"avg_NO\", F.avg(\"temp\").over(not_ordered)).withColumn(\n",
    "    \"avg_O\", F.avg(\"temp\").over(ordered)\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1869ee-da02-40f4-8196-296967fe943f",
   "metadata": {},
   "source": [
    "#### Two main types of window frames\n",
    "When you look at the documentation for [Window](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.html?highlight=window#pyspark.sql.Window) you find that the class creates the Window with one of two frames:\n",
    "\n",
    "1. An unbounded window frame if ordering is not applied; `rowframe` \n",
    "2. A growing window frame if ordering is applied; `rangeFrame`\n",
    "\n",
    "definition: window frame; the boundaries of a window.\n",
    "\n",
    "| year | mo | da | temp | boundary                |\n",
    "|------|----|----|------|-------------------------|\n",
    "| 2018 | 02 | 21 | 16.1 | Window.unboundPreceding |\n",
    "| 2019 | 04 | 19 | 20.6 | ...                     |\n",
    "| 2019 | 06 | 12 | 23.4 | -1 (row before current) |\n",
    "| 2019 | 06 | 20 | 27.8 | Window.currentRow       |\n",
    "| 2019 | 06 | 30 | 25.8 | +1 (row after current)  |\n",
    "| 2019 | 07 | 19 | 24.7 | ...                     |\n",
    "| 2019 | 08 | 17 | 20.8 | Window.unboundFollowing |\n",
    "\n",
    "In the above window, we can move up to our first row, marked with the unboundPreceding attribute. We can move down the window until we come to the unboundFollowing attribute. The current row has the values: zero, one above -1, and one below 1. PySpark will assign useful values to both the unboundPreceding and unboundFollowing attributes.\n",
    "\n",
    "Knowing this, we can add boundaries to our windows specification and solve our conundrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece09e5-2166-4071-865a-a2e256cbe769",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_ordered: WindowSpec = Window.partitionBy(\"year\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.unboundedFollowing\n",
    ")\n",
    "ordered: WindowSpec = not_ordered.orderBy(\"temp\").rangeBetween(\n",
    "    Window.unboundedPreceding, Window.unboundedFollowing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a36dd-4c54-4f7e-97c8-ac62a2e0ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod_light.withColumn(\"avg_NO\", F.avg(\"temp\").over(not_ordered)).withColumn(\n",
    "    \"avg_O\", F.avg(\"temp\").over(ordered)\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e91dd-5aa4-49f2-89f4-9b72ba01f6ec",
   "metadata": {},
   "source": [
    "It is simple to determine if you need an ordered or unordered window. Aggregation functions like `sum` or `avg` do not care about the order of things. Ranking and analytical functions as `lead` or `lag` depend on ordered windows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043d0da-c096-4678-9cb6-305d744f43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod_light_p: DataFrame = (\n",
    "    gsod_light.withColumn(\"year\", F.lit(2019))\n",
    "    .withColumn(\n",
    "        \"dt\", F.to_date(F.concat_ws(\"-\", F.col(\"year\"), F.col(\"mo\"), F.col(\"da\")))\n",
    "    )\n",
    "    .withColumn(\"dt_num\", F.unix_timestamp(\"dt\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef8202-d5c0-4286-9e86-8707823927dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod_light_p.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab185d8-3a3b-45ce-934b-a757a5c52c03",
   "metadata": {},
   "source": [
    "To be able to compare the temperature of a given day with that of the average of the previous month\n",
    "and the month after, gsod_light needed to undergo some transformations. A date column `dt` was added. To create a number from that date for easy counting, `dt_num` was added as a Unix timestamp from the date. The Unix timestamp counts the number of seconds lapsed since January 1, 1970. To approximate the month preceding or following, we just need to calculate `60*60*24*30` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec18e3-fe1a-4a9d-bfc7-6e5bb5da5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_in_sec: int = 60 * 60 * 24 * 30\n",
    "one_month_before_and_after: WindowSpec = (\n",
    "    Window.partitionBy(\"year\").orderBy(\"dt_num\").rangeBetween(-month_in_sec, month_in_sec)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dbf9d6-c14b-43ad-8ef1-7f1c0fc7638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod_light_p.withColumn(\n",
    "    \"avg_count\", F.avg(\"count_temp\").over(one_month_before_and_after)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a771e1-5ad4-4200-a442-f5da556e8b3b",
   "metadata": {},
   "source": [
    "#### Range vs. rows\n",
    "\n",
    "For each record in the window, Spark computes the range boundaries based on the current row value from the field dt_num and determines the actual window the function will aggregate over. In effect, narrowing or growing the windows. If you use ranges, the actual value of a row is used, not the row number. You make the window respect the context you are applying it over.\n",
    "\n",
    "We know six types of window types in PySpark:\n",
    "\n",
    " - Rows bounded; the window stays the same size and moves from record to record. The window is based on the position of the row. Numerical values are relative to the position of the current row.\n",
    " - Range bounded; the window stays the same size and moves from record to record. The window is based on the value of each row (e.g., 1555538400 in the above example). Numerical values are relative to the value of the current row (e.g., 1555536400 in the above example is one value from the current row).\n",
    " - Rows growing; the window grows and shriks in the direction in which it is not bound. The window is based on the position of the row. Numerical values are relative to the position of the current row.\n",
    " - Range bounded; the window grows and shriks in the direction in which it is not bounded. The window is based on the value of each row (e.g., 1555538400 in the above example). Numerical values are relative to the value of the current row. 1555536400 in the above example is one value from the current row if the window is unbounded relative to `Window.unboundPreceding`.\n",
    " - Rows unbounded; the window contains the whole partition. It stays the same for every record in the partition. The window stays the same size and moves from record to record. The window is based on the position of the row. Numerical values are relative to the position of the current row.\n",
    " - Range unbounded; the window contains the whole partition. It stays the same for every record in the partition. The window is based on the value of each row (e.g., 1555538400 in the above example). Numerical values are relative to the value of the current row (e.g., 1555536400 in the above example is one value from the current row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89398088-7e3d-40b3-bc61-3f9dc2276ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(\"double\")\n",
    "def median(vals: pd.Series) -> float:\n",
    "    return vals.median() \n",
    "\n",
    "spec: WindowSpec = Window.partitionBy(\"year\")\n",
    "\n",
    "gsod_light.withColumn(\n",
    "    \"median_temp\", median(\"temp\").over(spec)).withColumn(\n",
    "        \"median_temp_bounded\", median(F.col(\"temp\")).over(spec.orderBy(\"mo\", \"da\")) \n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c7b983-0e2d-4fd0-82cc-cd5ab0f1d827",
   "metadata": {},
   "source": [
    "As you can see, we can do a window with our own defined function, or in this case, the `median` \n",
    "function from Pandas. More on this in the next notebook on RDDs and user-defined functions.\n",
    "\n",
    "#### The main steps to a successful window function are:\n",
    "\n",
    "1. What kind of operation do I want to perform? Summarise, rank, or look ahead/behind.\n",
    "2. How do I construct my window? Should it be bounded or unbounded? In other words, do I need every record to have the same window value (unbounded) or should the answer depend on where the record fits in the window (bounded)? When bounding a window frame more often than not, you often order it as well.\n",
    "3. If you have a bounded window, do you want to bind the window according to the position of the record (row based) or the value of the record (range based)\n",
    "4. A window function does not make your DataFrame special; you can still filter it, use a group by, or apply another different window function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3940391-cc8b-4f4e-a545-2bf2d8298274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
