{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b02fede-72d5-4e74-9baf-740cca4055a8",
   "metadata": {},
   "source": [
    "## SQL operations in PySpark using Python\n",
    "\n",
    "With the advent of the [PySpark.sql API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html) and the DataFrame data-structure, the relationship between SQL and the PySpark api has become obvious. Where in the introduction notebook we looked at different ways of selecting columns, had a closer look at columns, but also investigated how to write good Python. In this notebook we will look at how to perform SQL operations using PySpark & Python.\n",
    "\n",
    "Before we continue looking at PySpark Python equivalents to SQL, we should quickly answer two questions:\n",
    "\n",
    "1. Can we use SQL instead of Python? \n",
    "2. Should we use SQL instead of Python?\n",
    "\n",
    "The short answer are yes and no. To eleborate why should choose to use Python instead of SQL. In the introduction notebook I gave a few reasons why we should use Python, easy to learn, great ecosystem of libraries, PySpark being one of them, and flexible in use. I also mentioned the biggest drawback to Python; Python is prone to mistakes, or better it is easy to write faulty code in Python. I presented you with some steps to write better code:\n",
    "\n",
    "1. Type your variables, functions, and classes.\n",
    "2. Use good names for your variables, functions, and classes\n",
    "3. Document your variables, functions, and classes using PyDoc and docstring\n",
    "4. Test your methods, using DocTest (Ideal for interactive code) \n",
    "\n",
    "SQL as language is less powerful than Python is, you can do more with Python. For instance you can do graphics with Python, you cannot do that with SQL. Where Python is easy to read, SQL can be notoriously hard to read with subqueries, views, and/or multiple joins. SQL is more error prone than Python. The most important reason why you should use Python instead of SQL; Python is very good at breaking up problems into smaller easier to solve problems, which you can than combine in to a solution, SQL is not. The one thing going for SQL is that it is in general faster than Python. However, we can speed up Python to match SQL if necessary.  \n",
    "\n",
    "As with the previous notebook, this notebook continuous with PySpark Python equivalents to SQL. That means looking at how to perform joins, order, group things, and using aggregate functions. \n",
    "\n",
    "We start with the same broadcast logs as the base DataFrame. ALL CSV files can be found at: [GitHub](https://github.com/jonesberg/DataAnalysisWithPythonAndPySpark-Data/tree/trunk/broadcast_logs)\n",
    "\n",
    "You have to remember to change the path if you run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d7479-a0f5-4c26-ba40-fc87a70a6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell for imports\n",
    "\n",
    "import doctest\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.errors import AnalysisException\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78c45c-5019-4929-9415-7bdc4b8fe4fd",
   "metadata": {},
   "source": [
    "Creating a session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79906497-6bb6-4d72-b282-28390c98df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark: SparkSession = SparkSession.builder.appName(\"Python Joins\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d36820-e192-4b6b-89b9-60adf3b754bd",
   "metadata": {},
   "source": [
    "As the first part of this notebook is first and foremost about joins we will need two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2bf761-66e3-4634-86e0-05240574e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_one: str = \"./Downloads\"\n",
    "broadcast_logs: DataFrame = spark.read.csv(\n",
    "    path=os.path.join(path_one, \"BroadcastLogs_2018_Q3_M8.CSV\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    timestampFormat=\"yyyy-MM-dd\",\n",
    ")\n",
    "\n",
    "path_two = \"./ProgrammingProjects/SparkTest/DataAnalysisWithPythonAndPySpark-Data-trunk/broadcast_logs/ReferenceTables/\"\n",
    "\n",
    "log_identifier: DataFrame = spark.read.csv(\n",
    "    path=os.path.join(path_two, \"LogIdentifier.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b97301-7ca4-4bf2-9a9b-9f7351274f56",
   "metadata": {},
   "source": [
    "A quick inspection of `log_identifier`, we have used `broadcast_logs` before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c8ed0-a094-4c03-aeb3-9b9f035361f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_identifier.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a304d6-571b-4fa0-9d99-d3feac7b2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_identifier.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a5275-a551-4fbd-be98-01e926abeed5",
   "metadata": {},
   "source": [
    "## The different joins in PySpark\n",
    "\n",
    "The purpose of a join is to answer one of two questions: what happens when the return value of the predicate is true, and conversely.\n",
    "\n",
    "#### The types of joins\n",
    "\n",
    "1. The inner join returns the record if the predicate is true; otherwise, it drops it. Standard join option.\n",
    "2. The left/right inner join will join the unmatched records from the left/right, filling the columns from the right/left with NULL.\n",
    "3. The full outer join: the fusion from the left and right inner join will add the unmatched records from the left and right padding with NULL.\n",
    "4. The left semi join: same as an inner join, it keeps the columns on the left but will discard those rows that fulfil the predicate with more than one record from the right.\n",
    "5. The left anti-join keeps only those records from the left that do not match with the predicate; it will drop all those that do.\n",
    "6. A Cartesian product, or cross-join. Pyspark has a specific `crossJoin()` method. Obviously, using this method will explode your DataFrame into enormous proportions!\n",
    "\n",
    "All these different joins are parameter values in the `join` method, with the key word \"how\" as in how=inner. Inner is the standard option in the `join` method.\n",
    "\n",
    "Performance-wise, you will need to have both tables on the same computer; if not, PySpark will perform the join on the network, which will quickly cost you >99% of performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dba59a-4086-4d2f-84c4-f396dd60506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_and_channels_verbose: DataFrame = broadcast_logs.join(\n",
    "    other=log_identifier,\n",
    "    on=broadcast_logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"],\n",
    "    how=\"inner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a6b00-9dc3-47c8-a4e6-a278e4a44b1c",
   "metadata": {},
   "source": [
    "A join performed like this will work, but will lead to ambiguity, see the following error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91112492-184d-4655-9e23-ca05afc9fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logs_and_channels_verbose.select(\"LogServiceID\")\n",
    "except AnalysisException as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f78a36-3922-4058-8a9c-864f7984ed17",
   "metadata": {},
   "source": [
    "If you print the schema you will see the two LogServiceIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b2c03-5e9d-4761-a67d-d28d5cd622ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_and_channels_verbose.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bb5b5-9694-4e52-a699-a05ae3f1e968",
   "metadata": {},
   "source": [
    "We can make the join without the `==` and drop the offending column in one swoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3fc6a-f435-4982-8d2a-ba2cfb1eaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_and_channels = broadcast_logs.join(\n",
    "    other=log_identifier, on=\"LogServiceID\", how=\"inner\"\n",
    ").drop(log_identifier[\"LogServiceID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e00804-c3f1-4f81-83b8-1abcb9fc3748",
   "metadata": {},
   "source": [
    "Now we have only one  column \"LogServiceID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b1697-aff0-4fcd-8277-58a6b2388e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([col for col in logs_and_channels.columns if col == \"LogServiceID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d99f3b-2dd5-43f6-8410-c05164c2fd3a",
   "metadata": {},
   "source": [
    "We can join several tables in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9293b6-8354-4fbd-9d3e-9e1de1296eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = (\n",
    "    \"./ProgrammingProjects/SparkTest/DataAnalysisWithPythonAndPySpark-Data-trunk/broadcast_logs/\"\n",
    ")\n",
    "\n",
    "cd_category: DataFrame = spark.read.csv(\n",
    "    path=os.path.join(path, \"ReferenceTables/CD_Category.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"CategoryID\",\n",
    "    \"CategoryCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"Category_Description\"),\n",
    ")\n",
    "\n",
    "cd_program_class: DataFrame = spark.read.csv(\n",
    "    path=os.path.join(path, \"ReferenceTables/CD_ProgramClass.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"ProgramClassID\",\n",
    "    \"ProgramClassCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"ProgramClass_Description\"),\n",
    ")\n",
    "# joining 3 tables together\n",
    "full_log: DataFrame = logs_and_channels.join(\n",
    "    other=cd_category, on=\"CategoryID\", how=\"left\"\n",
    ").join(other=cd_program_class, on=\"ProgramClassID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833582a3-1e5b-499f-a448-8347995e0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4a45a-290c-47c2-9404-91c82d31a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_log.select(\"ProgramClassCD\", \"ProgramClass_Description\", \"Duration\").show(\n",
    "    n=5, truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c11a27-41d5-4183-a062-24dc45e7146d",
   "metadata": {},
   "source": [
    "The duration is very unclear; we would want to use the same base, seconds in this case.\n",
    "Because we are using `F.col` we can use the [`substr`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.substr.html#pyspark.sql.Column.substr) method from the [column API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html) (one of PySpark core classes), we can transform duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ba2e2-d1bb-4091-8e69-89e7ee174b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_log: DataFrame = full_log.withColumn(\n",
    "    \"duration_seconds\",\n",
    "    (\n",
    "        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 3600\n",
    "        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
    "        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cc8ef-c893-4e43-b4fc-884a9acd0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_log.select(\"ProgramClassCD\", \"ProgramClass_Description\", \"duration_seconds\").show(\n",
    "    n=5, truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f40f1-d578-4071-93fa-00cb6f869d07",
   "metadata": {},
   "source": [
    "#### Grouping, Ordering, and Having \n",
    "The `groupBy` and `orderBy` functions do pretty much what you would expect from knowing SQL. However, there is a major difference in PySpark: the return value:\n",
    "\n",
    "- `DataFrame.groupBy → DataFrameGroupBy`\n",
    "- `DataFrame.orderBy → pyspark.sql.dataframe.DataFrame`\n",
    "\n",
    "We would need to perform an operation on the DataFrameGroupBy to get a DataFrame. Also, you will see both `groupby` and `groupBy` but you won't see `orderby` just `orderBy`. To avoid confusing myself, I just use the capitalised B for both methods. \n",
    "\n",
    "As having is just filtering after grouping there is no Python equivalent, just use `filter` or `where`, the latter being alias for the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79c401-1181-4c23-84bb-ba1a775136a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_log.groupBy(\"ProgramClassCD\", \"ProgramClass_Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cedd65-f408-47b0-8809-cd0f842797ef",
   "metadata": {},
   "source": [
    "## aggregating\n",
    "\n",
    "The `agg` method totals on a given axis and returns a DataFrame, we can use it on GroupedData. The `agg` method takes an aggregation function. Usually one of the built-in aggregation functions, such as avg, max, min, sum, and count.\n",
    "\n",
    "The [agg](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg) method is an important method; I advise you to look at the documentation. \n",
    "\n",
    "The `agg` is also a method of the DataFrame class that aggregates on the entire DataFrame without grouping. Instead of using a `F.sum(column)`, we could have added a dictionary to `agg`; with the column we want to aggregate over and the function we want to use: `agg({'duration_seconds':'sum'})`. However, that would prevent us from aliasing the column, and we would have to rename it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3720e-9c66-47b7-9501-8d0d8000d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_one: DataFrame = (\n",
    "    full_log.groupBy(\"ProgramClassCD\", \"ProgramClass_Description\")\n",
    "    .agg(F.sum(\"duration_seconds\").alias(\"duration_total\"))\n",
    "    .orderBy(\"duration_total\", ascending=False)\n",
    ")\n",
    "analysis_one.show(n=20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf05ae28-0eaa-46cc-b2e4-dfb232b1bfda",
   "metadata": {},
   "source": [
    "You now have another DataFrame you could use for analysis or report with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751c11a-8337-4122-bae2-d0ea16dabc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_one.select(\"duration_total\").summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e4e75b-4f8d-4827-a05d-5e6dea162743",
   "metadata": {},
   "source": [
    "## Mixing Python with SQL\n",
    "\n",
    "\n",
    "Spark knows two SQL dialects: the ANSI standard SQL and HiveQL, Hive is an open-source data warehouse. This notebook will use ANSI SQL and some built-in SQL functions that PySpark provides. Databricks provides you with the warehouse, so there is no need for Hive.\n",
    "\n",
    "**Declarative programming with SQL**\n",
    "\n",
    "SQL is a declarative language; programmes describe their desired results without explicitly listing commands or steps that must be performed. The DML (data manipulation language) part of SQL basically splits the programmes into two parts:\n",
    "\n",
    "- Operations, comprising select columns from a target and functions such as count, max, but also aliasing; `select whatever as alias, count(*) from target`\n",
    "- Conditions: comprising  conditions where or having, grouping, ordering, and filtering; `where condition1 and condition 2, group by condition1 having something order by`\n",
    "\n",
    "**Imperative programming with Python**\n",
    "\n",
    "Python is an imperative programming language. In short, some object represents the state of a computer program, and the program gives instructions on how to change that state. The standard state in PySpark is the DataFrame, PySpark chains the transformations and actions on that DataFrame.\n",
    "\n",
    "SQL is faster than Python. SQL is native to Spark, and the data Spark uses is almost always tabular. SQL, which is an application of relational algebra, is a domain-specific language; SQL is made for relational data. Whereas Python can handle relational data but is not made for it, which is why Python programmers usually use an ORM like SQLAlchemy.\n",
    "\n",
    "Why use Python at all? Like all imperative languages, Python can easily breakdown large problems into small problems, which can be built up in an algorithmic manner to solve your larger problems. \n",
    "\n",
    "I will start this notebook with a bit of Python. As I said, we first need to get a state we can change. In PySpark, the state is a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e58e3a-028b-404a-ab1f-85456e481788",
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = (\n",
    "    \"./ProgrammingProjects/SparkTest/DataAnalysisWithPythonAndPySpark-Data-trunk/\"\n",
    ")\n",
    "\n",
    "elements: DataFrame = spark.read.csv(\n",
    "    path=path + \"elements/Periodic_Table_Of_Elements.csv\", header=True, inferSchema=True\n",
    ")\n",
    "elements.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b74797-f9be-49cf-8d9c-03037dc303ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements.select(\"AtomicNumber\", \"Element\", \"MeltingPoint\", \"BoilingPoint\").show(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1e911-c67e-40e2-a191-c8ac163eb695",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements.filter(F.col(\"AtomicNumber\") == 79).select(\n",
    "    \"AtomicNumber\", \"Element\", \"MeltingPoint\", \"BoilingPoint\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96f7bb-4eba-4c6e-a4b8-196c03ffec3e",
   "metadata": {},
   "source": [
    "Say we wanted to get following query in SQL:   \n",
    "\n",
    "```\n",
    "SELECT\n",
    "  element,\n",
    "  period,\n",
    "  count(*)\n",
    "FROM elements\n",
    "WHERE phase='gas'\n",
    "GROUP BY period;\n",
    "```\n",
    "\n",
    "We could write the following line of code in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e70824-c31a-4c8e-bf81-001265566a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements.where(F.col(\"Phase\") == \"gas\").groupBy(\"period\").count().show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4009236-e013-4b9e-9ae8-e122bcdda52d",
   "metadata": {},
   "source": [
    "The PySpark `groupBy` automatically selects the column. A short cut, if\n",
    "you only want a few columns.  \n",
    "\n",
    "We could use the SQL too, despite having a DataFrame as datastructure. We need to create a `view`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5775f6-278d-4e3f-a62d-896b0da658a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements.createOrReplaceTempView(name=\"elements\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "  period,\n",
    "  count(*)\n",
    "FROM elements\n",
    "WHERE phase='gas'\n",
    "GROUP BY period;\n",
    "\"\"\"\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45364f-d05d-406e-b20c-1d8c736f111d",
   "metadata": {},
   "source": [
    "You might have noticed we have not assigned a name, so where does PySpark store this view? \n",
    "Within the session is the short answer. We can inspect the sessions' [catalog](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Catalog.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccdba52-5b69-4d58-855d-19cbfa0d13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7afc431-b034-4249-bb55-d31d3e2602d7",
   "metadata": {},
   "source": [
    "#### View vs. Table\n",
    "\n",
    "A quick side note on the difference between a view and a table. The important difference is that a table is an existing entity in the database or data warehouse, whereas a view is the instructions on how to create the table. A view is therefor also known as a virtual table.\n",
    "\n",
    "You will notice this difference in use: a table can be placed directly in memory (if the memory available is big enough), and a view needs to be computed every single time a session is started. This has an immediate effect on performance. Operations on a table are faster.\n",
    "\n",
    "Why use a view? If you want to create a table from different tables but intend to use it sparsely, it wouldn't be worth creating an actual object in persistent storage for it. Then you should use a view.\n",
    "\n",
    "PySpark and views are slightly different; you have already created the object (the DataFrame you are referencing) when making it a view. Performance-wise, this would be on par with loading a table in memory.\n",
    "\n",
    "A view created with `createOrReplaceTempView` only exists as long as the DataFrame exist, that is, for the duration of the session.\n",
    "\n",
    "A view created with `createOrReplaceGlobalView` exists as long as there is a Spark application. This is really only necessary if you have an application that requires multiple SparkSessions to cooperate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8603b-9929-42b8-8ea2-0e265bf49c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1: DataFrame = spark.read.csv(path=\"./drive_stats_Q1\", header=True, inferSchema=True)\n",
    "q2: DataFrame = spark.read.csv(path=\"./drive_stats_Q2\", header=True, inferSchema=True)\n",
    "q3: DataFrame = spark.read.csv(path=\"./drive_stats_Q3\", header=True, inferSchema=True)\n",
    "q4: DataFrame = spark.read.csv(path=\"./drive_stats_Q4\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282cca3e-033a-4096-8708-f701aaee95f2",
   "metadata": {},
   "source": [
    "## Backblaze\n",
    "Backblaze is a cloud storage provider that, among other things, offers a service for IaaS. For instance, for storage in the cloud. Backblaze offers you many hard disk drive options; how would you know which one to choose? You want the best fitting capacity with the lowest failure rate. PySpark can determine this.\n",
    "\n",
    "For the following programming examples, you will need to download the quarterly files for 2019 (q1–q4) from [Backblaze](https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data).\n",
    "\n",
    "You need to unpack all the folders; PySpark has no automatic unzipping. Furthermore, you need to use your own paths if you run the code in the notebook.\n",
    "\n",
    "I happen to know that Q4 has more columns than the other quarters.  This information you won't get from calling `printSchema`,  `show`, or `count`. But we can revert to simple Python if we want to know the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa3788-bd1d-4d30-9e1d-2b76271b7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q3.columns) - len(q4.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9aaa1-1436-4cc0-8bce-3429c66165ec",
   "metadata": {},
   "source": [
    "#### Python helpers\n",
    "Now we could imagine wanting to create a single DataFrame from multiple CSV files quite often. This is where Python comes in. The advantage of using Python over SQL is that Python is a much more powerful language than SQL. In technical terms, Python is Turing complete; SQL is not. For instance, instead of manually having to ask and compare the size of the columns in a table or DataFrame, we can write helper functions that we can use over and over again. \n",
    "\n",
    "Normally, I would put these helper functions in a separate module or even a class (the CSV Fusion class?) and not in the script. If you import the module with your imports, you will always have access to a host of useful helper functions. Save yourself the hassle of repeatedly having to write the same or similar code.\n",
    "\n",
    "As per the 4 steps these helpers should be typed, have names that convey their use, they should be documented, and they should be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa9593-a3f8-4d95-ba8e-fecea54b5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_column_size(dfs: list[DataFrame]) -> bool:\n",
    "    \"\"\"\n",
    "    desc: Function to compare column size\n",
    "    >>> same_column_size([q1,q2])\n",
    "    True\n",
    "    >>> same_column_size([q1,q4])\n",
    "    False\n",
    "    \"\"\"\n",
    "    size = len(dfs[0].columns)\n",
    "    for df in dfs:\n",
    "        if len(df.columns) != size:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def compare_column_size(dfs: list[DataFrame]) -> list[int]:\n",
    "    \"\"\"\n",
    "    desc: Function that returns the column sizes\n",
    "    >>> compare_column_size([q1,q2,q3,q4])\n",
    "    [129, 129, 129, 131]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(len(dfs)):\n",
    "        result.append(len(dfs[i].columns))\n",
    "    return result\n",
    "\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b5281-15dd-48a2-9ea4-474fc033acce",
   "metadata": {},
   "source": [
    "Now that we know that one quarter has more columns, we should either drop those columns \n",
    "or add them to the other quarters. We are going to do the latter. Again, this is something that you could consider doing in a separate module or in a class. At the end of this notebook, I will show you a different way to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f74843-df5e-4ccc-a7a7-27bb248f0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "q4_columns_extra: set[str] = set(q4.columns) - set(q1.columns)\n",
    "\n",
    "# add the columns, fill them with None value\n",
    "for col in q4_columns_extra:\n",
    "    q1 = q1.withColumn(col, F.lit(None).cast(T.StringType()))\n",
    "    q2 = q2.withColumn(col, F.lit(None).cast(T.StringType()))\n",
    "    q3 = q3.withColumn(col, F.lit(None).cast(T.StringType()))\n",
    "\n",
    "# creating the full set of data (equivalent to an SQL union all)\n",
    "back_blaze_2019: DataFrame = (\n",
    "    q1.select(q4.columns)\n",
    "    .union(q2.select(q4.columns))\n",
    "    .union(q3.select(q4.columns))\n",
    "    .union(q4)\n",
    ")\n",
    "\n",
    "# setting the layout for each column according to the schema\n",
    "back_blaze_2019: DataFrame = back_blaze_2019.select(\n",
    "    [\n",
    "        F.col(x).cast(T.LongType()) if x.startswith(\"smart\") else F.col(x)\n",
    "        for x in back_blaze_2019.columns\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45834b63-4850-4798-ac09-1bf31f1de07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a view for the SQL\n",
    "back_blaze_2019.createOrReplaceTempView(\"backblaze_2019_view\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587554cb-f327-473e-aa11-03662c28058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT model, serial_number \n",
    "    FROM backblaze_2019_view \n",
    "    where failure = 1\n",
    "    \"\"\"\n",
    ").show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f51148-bc5e-4b14-80f5-fae068652ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "back_blaze_2019.where(\"failure=1\").select(\"model\", \"serial_number\").show(\n",
    "    n=5, truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190697ea-0dd4-45f3-94d0-7aab92a4d594",
   "metadata": {},
   "source": [
    "Placeholders can create unclear code if used thoughtless. Compare the queries and determine which is \n",
    "clearer from the point of view of the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d7977-f0fc-4df4-8c28-1bd6773ce6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT model, min(capacity_bytes / pow(1024, 3)) as min_GB, max(capacity_bytes / pow(1024, 3)) as max_GB\n",
    "    FROM backblaze_2019_view\n",
    "    GROUP BY 1\n",
    "    ORDER BY 3 DESC\n",
    "    \"\"\"\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d8b69-2ea5-4337-adef-7437ca4d5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT model, min(capacity_bytes / pow(1024, 3)) as min_GB, max(capacity_bytes / pow(1024, 3)) as max_GB\n",
    "    FROM backblaze_2019_view\n",
    "    GROUP BY model\n",
    "    ORDER BY max_GB DESC\n",
    "    \"\"\"\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b835d-b720-4361-8aca-07227b56d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "back_blaze_2019.groupBy(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(1024, 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(1024, 3)).alias(\"max_GB\"),\n",
    ").orderBy(F.col(\"max_GB\"), ascending=False).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484e03b-726e-4d48-82ea-a888d07aee68",
   "metadata": {},
   "source": [
    "#### The having clause\n",
    "What if you want to filter after grouping? In SQL, you have the having clause. In Python, you just use the `filter` function or its alias `where` after the grouping. Just remember that `groupBy` returns a grouping object. You need a DataFrame to use `filter`. In general, this won't be a problem, as you will want to use some aggregation over the group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb35dee-4025-4c87-904d-75285146e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT model, min(capacity_bytes / pow(1024, 3)) as min_GB, max(capacity_bytes / pow(1024, 3)) as max_GB\n",
    "    FROM backblaze_2019_view\n",
    "    GROUP BY model\n",
    "    HAVING min_GB != max_GB\n",
    "    ORDER BY max_GB DESC\n",
    "    \"\"\"\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111443a1-2d77-4b46-a34f-40eb396b4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent Python where first group then filter\n",
    "back_blaze_2019.groupBy(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(1024, 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(1024, 3)).alias(\"max_GB\"),\n",
    ").filter(F.col(\"min_GB\") != F.col(\"max_GB\")).orderBy(\n",
    "    F.col(\"max_GB\"), ascending=False\n",
    ").show(\n",
    "    n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847bb774-d26b-4253-923b-be411c05ed8f",
   "metadata": {},
   "source": [
    "#### data definition language (DDL)\n",
    "\n",
    "Sofar we have focussed on using the DML part of SQL. We can equally use DDL in PySpark. Let me first create a new view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b1734-7056-40d5-ac23-b5241dee98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_blaze_2019.createOrReplaceTempView(\"drive_stats_view\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21360ab8-0301-4780-a155-d81a9e2903d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW drive_days AS\n",
    "    SELECT model, count(*)  AS drive_days\n",
    "    FROM drive_stats_view\n",
    "    GROUP BY model\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW failures AS\n",
    "    SELECT model, count(*) AS failures\n",
    "    FROM drive_stats_view\n",
    "    WHERE failure = 1\n",
    "    GROUP BY model\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6cd5f8-d640-4d7e-9287-55dadb597bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e11cbc2-0118-452c-8e28-86a2e7628f9e",
   "metadata": {},
   "source": [
    "We have added two views to our catalog.\n",
    "\n",
    "Back to the difference between a view and a table: A table is an actual object in persistent memory. A Python object, for instance, a DataFrame, is not stored in persistent memory. It exists very much as a set of instructions (the class DataFrame) on how to make something and only goes into memory once compiled. A view is thus much more equal to a Python object than a table is. \n",
    "\n",
    "Of course, we know how to do very much the same in Python as we just did in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335037a-2656-4d99-8a4f-5c803213df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_days: DataFrame = back_blaze_2019.groupBy(F.col(\"model\")).agg(\n",
    "    F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    ")\n",
    "drive_days.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55698492-60bd-41d0-8f9d-4089ed5a25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures: DataFrame = (\n",
    "    back_blaze_2019.filter(\"failure == 1\")\n",
    "    .groupBy(\"model\")\n",
    "    .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    ")\n",
    "failures.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9110ea-4e2d-475a-8d22-30ec74b0e896",
   "metadata": {},
   "source": [
    "## Relational Operators\n",
    "Python knows relational operators, such as `union` and `intersect_update`. As a table is a set of rows, we can use the set relational operators in Spark too. Given sets **A** and **B**, these operators are:\n",
    "1. Union. $A \\cup B = \\ x \\in A \\lor x \\in B$. In practice, you will probably mostly see the **union**.\n",
    "2. Intersect. $A \\cap B = x \\in A \\land x \\in B$  \n",
    "3. Except. $A / B =  x \\in A, x\\ni B$\n",
    "\n",
    "It is important that when you use a set operator on two tables, they have the same number of columns and that those columns have comparable data types! \n",
    "\n",
    "Also, you should be aware that there is a difference between a **SQL union**, which removes duplicates, and a **PySpark union**, which does not do so. There is a good reason why the PySpark union does not remove duplicates; this is a very computationally expensive operation in a distributed environment. This is why you should not use plain union in SQL but the **union all** in distributed environments.\n",
    "\n",
    "Why do you need this? Simply because you quite often want to join the data from multiple tables in a bigger table. We have done this above using Python, i.e., `.union(q2.select(q4.columns))` Now we will do so in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4decd35f-3dad-41be-9806-657b0774e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a string for use in sql\n",
    "columns_backblaze = \", \".join(q4.columns)\n",
    "\n",
    "# create the views\n",
    "q1.createOrReplaceTempView(\"Q1\")\n",
    "q2.createOrReplaceTempView(\"Q2\")\n",
    "q3.createOrReplaceTempView(\"Q3\")\n",
    "q4.createOrReplaceTempView(\"Q4\")\n",
    "\n",
    "# create the same view as the backblaze_2019 DataFrame\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW backblaze_2019 AS\n",
    "    SELECT {col} from Q1 UNION ALL\n",
    "    SELECT {col} from Q2 UNION ALL\n",
    "    SELECT {col} from Q3 UNION ALL\n",
    "    SELECT {col} from Q4\n",
    "    \"\"\".format(\n",
    "        col=columns_backblaze\n",
    "    )\n",
    ")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f09efa-34c8-402d-8f5e-ca93bf6e9b6f",
   "metadata": {},
   "source": [
    "#### Union vs. Join\n",
    "In my mind, there always seems to be a bit of confusion between a **join** and a **union**. A union basically adds rows to a column, and a join adds columns to tables; it makes a cartesian product and subsets it, to be exact.\n",
    "\n",
    "Of course, we can use join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed4cde-d880-48ae-a3e2-8ef0707312e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT drive_days.model, drive_days, failures\n",
    "    FROM drive_days\n",
    "    LEFT JOIN failures\n",
    "    ON drive_days.model = failures.model\n",
    "    \"\"\"\n",
    ").show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235875bb-8445-4d0c-b806-3d7a01a79c01",
   "metadata": {},
   "source": [
    "In Python, we can achieve the same, but in my opinion, cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefab6f-af36-4c95-a3e9-5c6c1e0fb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_days.join(other=failures, on=\"model\", how=\"left\").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b91e5-3b25-4131-ae8d-e280748abed4",
   "metadata": {},
   "source": [
    "## Complex queries\n",
    "Now we come to why I think that Python is the better choice for PySpark. Though SQL is enormously powerful for handling tabular data, it is very hard to structure SQL. SQL tends to become quickly difficult to read, especially when you start using subqueries and subqueries in subqueries.\n",
    "\n",
    "Consider the following somewhat complex SQL query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fea961-ac99-49e5-98c8-4ce021c1e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT failures.model, failures / drive_days AS failure_rate\n",
    "    FROM (\n",
    "        SELECT model, count(*) AS drive_days\n",
    "        FROM drive_stats_view\n",
    "        GROUP BY model\n",
    "        ) drive_days\n",
    "    INNER JOIN (\n",
    "        SELECT model, count(*) AS failures\n",
    "        FROM drive_stats_view\n",
    "        WHERE failure = 1\n",
    "        GROUP BY model\n",
    "        ) failures\n",
    "    ON drive_days.model = failures.model\n",
    "    ORDER BY failure_rate DESC\n",
    "    \"\"\"\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79a427-578e-44aa-9c06-103db92d219f",
   "metadata": {},
   "source": [
    "In Python, we can breakdown this problem into three separate steps:\n",
    "\n",
    "1. Create a drivedays DataFrame\n",
    "2. Create a failures DataFrame\n",
    "3. Create a failure_rates DataFrame by joining the first two.\n",
    "   \n",
    "I have created the first two already, but let's copy them in for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9510d38-59d5-4d89-b892-655a53325680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "drive_days: DataFrame = back_blaze_2019.groupBy(F.col(\"model\")).agg(\n",
    "    F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    ")\n",
    "# step 2\n",
    "failures: DataFrame = (\n",
    "    back_blaze_2019.filter(\"failure == 1\")\n",
    "    .groupBy(\"model\")\n",
    "    .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    ")\n",
    "# step 3\n",
    "failure_rates: DataFrame = (\n",
    "    drive_days.join(other=failures, on=\"model\", how=\"inner\")\n",
    "    .withColumn(\n",
    "        colName=\"failure_rate\", col=F.round(F.col(\"failures\") / F.col(\"drive_days\"), 5)\n",
    "    )\n",
    "    .orderBy(F.col(\"failure_rate\").desc())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab788518-3bfc-48ba-b530-652d307d5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_rates.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c970c5-873c-44a2-a0ee-dc8c425a8871",
   "metadata": {},
   "source": [
    "Of course, SQL has common table expressions that you can use to break down complex code. However,\n",
    "I feel that CTEs do not really make code less complex. See the example using CTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c25bb-ab4a-4fdd-8c6d-f0106117648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    WITH drive_days as (\n",
    "        SELECT model, count(*) AS drive_days\n",
    "        FROM drive_stats_view\n",
    "        GROUP BY model\n",
    "        ),\n",
    "         failures as (\n",
    "        SELECT model, count(*) AS failures\n",
    "        FROM drive_stats_view\n",
    "        WHERE failure = 1\n",
    "        GROUP BY model\n",
    "        )\n",
    "    SELECT failures.model, failures / drive_days AS failure_rate\n",
    "    FROM drive_days\n",
    "    INNER JOIN failures\n",
    "    ON drive_days.model = failures.model\n",
    "    ORDER BY failure_rate DESC\n",
    "    \"\"\"\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe1a219-08a9-4710-9f6d-eaf01852e24c",
   "metadata": {},
   "source": [
    "#### Namespaces\n",
    "There is a major disadvantage to the Python solution I presented. Our intermediary objects (drive_days and failures) are still available. This is not what we want; they were mere steps in our solution. In Python technical terms, they are in the general [namespace](https://github.com/lausandt/Programming-in-Python-notebooks/blob/master/StructuredProgramming.ipynb). A namespace is a mapping of names to objects; in Python, these namespaces take the form of a dictionary. To see all the objects in the namespace, just call `dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4631b-7535-4ef8-9fcc-4ed7bb23a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff93f865-bc17-452d-916d-0893552fbe01",
   "metadata": {},
   "source": [
    "#### Secure Python solution\n",
    "As you can see, our intermediary results are in the general namespace of the script. Available for all that have access to change and, with that, to change our answer. What we should do is encapsulate the intermediary results in their own namespace. How? Simple: we encapsulate the calculations with a function, narrowing the namespace of the dataframes to the function.\n",
    "\n",
    "This function is too complicated to test with DoctTest and would require some mocking of test data for the test. Complicated functions like this should be written in a separate script and tested with a test framework like [Pytest](https://docs.pytest.org/en/8.2.x/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a7556-76d7-4f82-9b43-011f6e857b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def failure_rate(drive_stats: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    desc:Function to show the failure rate of Hard Disk Drives\n",
    "    \"\"\"\n",
    "    drive_daysA = drive_stats.groupBy(F.col(\"model\")).agg(\n",
    "        F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    "    )\n",
    "    failuresA = (\n",
    "        drive_stats.filter(F.col(\"failure\") == 1)\n",
    "        .groupBy(F.col(\"model\"))\n",
    "        .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    "    )\n",
    "    answerA = (\n",
    "        drive_daysA.join(other=failuresA, on=\"model\", how=\"inner\")\n",
    "        .withColumn(\"failure_rate\", F.round(F.col(\"failures\") / F.col(\"drive_days\"), 5))\n",
    "        .orderBy(F.col(\"failure_rate\").desc())\n",
    "    )\n",
    "    return answerA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246912d7-1788-4315-8ed5-50593bb47220",
   "metadata": {},
   "outputs": [],
   "source": [
    "fails: DataFrame = failure_rate(drive_stats=back_blaze_2019)\n",
    "fails.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786679d1-8ce6-4660-bd9d-d6d1f346f1b5",
   "metadata": {},
   "source": [
    "You can check there are no sub results in the general namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d5b4d-e4be-44bc-a2a5-eef9cfe220dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"drive_daysA\" in dir() or \"failuresA\" in dir() or \"answerA\" in dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967b810-6c2f-4cfd-965e-37718cde4a83",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "This is a long notebook and there is more to say about PySpark, SQL, and Python. Those are the more advanced topics, such as:\n",
    "\n",
    "1. Using functional style programming\n",
    "2. SQL Expressions in PySpark\n",
    "3. Caching\n",
    "4. Windows functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd101f-39f1-4ef3-a6df-6baab6fe2799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
