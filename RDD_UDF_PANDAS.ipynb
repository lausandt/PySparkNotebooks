{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded47d05-ca8b-4c18-be51-815dbb6259e2",
   "metadata": {},
   "source": [
    "## Row like data (RDD), user defined functions (UDFs), and Pandas in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1406f72-514c-43a9-a769-c5597acc8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed pandas or pyarrow uncomment the line below or run it from the cli\n",
    "# pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5a6e4-8305-4eb1-9e7e-5cd13f5558d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell for imports\n",
    "import doctest\n",
    "import math\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial, reduce\n",
    "from numbers import Number\n",
    "from operator import add, mul, sub\n",
    "from typing import Any, Callable, Final, Iterator, Tuple, Generator\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pandas.core.frame import DataFrame as PandasDataFrame\n",
    "from pandas.core.series import Series\n",
    "from py4j.protocol import Py4JJavaError\n",
    "from pyspark.broadcast import Broadcast\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b17c5a-93b9-4a05-8ead-a4769139280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97080966-d522-46d8-ba4c-613a458270cd",
   "metadata": {},
   "source": [
    "#### RDD\n",
    "\n",
    "PySpark's DataFrame object can be manipulated with a host of methods. Spark will use those manipulations and create an optimised query plan for execution. However, a DataFrame has constraints; we cannot randomly determine what is in a column, for instance. PySpark dictates that all column entries are of the same data type. But what if our data has different types? What if we want to write our own Python functions to manipulate data?\n",
    "\n",
    "We can use the resilient distributed dataset (RDD). Though I presume that the DataFrame will be enough as the state you need for data manupilation, there are two use cases for the RDD:\n",
    "\n",
    "1. You have an unordered collection of Python objects that can be pickled.\n",
    "2. You have unordered key-value pairs like a Python dictionary.\n",
    "\n",
    "Now, I would not be surprised if you had never heard of [pickling](https://docs.python.org/3/library/pickle.html). A pickle is the Python method for serialising objects. Serialising objects is the process of translating an object's state into a format that can be stored, transmitted, and reconstructed. If you operate in a distributed environment (multiple computers at possibly multiple locations) and you want to perform manipulation of the state (remember, Python is an imperative programming language; it manipulates state; the object in Python is the representation of the state), you need to ensure that all computers that perform operations do this on the same object.\n",
    "\n",
    "I say all of this while thinking it will be nothing more than some useful background information, but it hardly gives you any insight into a RDD. To understand an RDD, we should go back to the DataFrame. The DataFrame is primarily about columns; the vast majority of the `pyspark.sql` API is about manipulating columns. One of the most important attributes of a column is that the elements in that column have to have the same type (int, string, etc.). The RDD does not have this constraint; it is basically a set theoretic bag*. You can mix types; you can have multiplicity in a bag. I use the set theory example because set theory is the basis upon which all relational algebra (SQL) is based. You can consider an RDD to be row-like, for in tabular data the rows can have multiple types. A Python list is akin to a set theoretic bag; let's use an example with a list.\n",
    "\n",
    "*Dutch mathematician Dick de Bruijn was the first to formalise the multiset/bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d4a61-6cfe-4add-8e8b-f9374fd1c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag: list[Any] = [1, \"two\", 3.0, (\"four\", 4), {\"five\": 5}]\n",
    "for e in bag:\n",
    "    print(type(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d857fd-eb9a-4eee-b0e0-d9023a431b8c",
   "metadata": {},
   "source": [
    "With this list, we can create an RDD using the `parallelize` method. This is an excellent name. \n",
    "So even if you did not know you had this skill, as Spark is all about parallel computing (not to be confused with concurrency), you know parallel computing. The hallmark of parallel computing is that you use multiple CPUs, most likely on different computers and perhaps in multiple locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ccc887-028c-4ba5-80b0-f0786b52d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_rdd: RDD = sc.parallelize(bag)\n",
    "bag_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa2682-389a-4533-a91c-3e510d7820cf",
   "metadata": {},
   "source": [
    "## MAP, FILTER, REDUCE\n",
    "With the RDD, we sort of leave Python and enter Scala territory, where we start manipulating the RDD datastructure with the use of three typical concepts from functional programming (Scala is a functional programming language):\n",
    "1. Map: You map a function to all elements in a collection.\n",
    "2. Filter: You apply a predicate to all elements of a collection and filter those for which the predicate is true.\n",
    "3. Reduce (fold): You apply a combining operator to the elements of a collection and recursively recombine the parts.\n",
    "\n",
    "These functions are enough to understand how to manipulate an RDD. You should understand, that these functions are higher-order functions; these functions take other functions as their argument. Let me give you some Python examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f8f8d-789c-4d3d-a32b-fe6cbb0854ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex: tuple[int] = *range(1, 5), 5\n",
    "\n",
    "\n",
    "def mult2(n: float | int) -> float | int:\n",
    "    \"\"\"\n",
    "    desc: Function that multiplies the input by two\n",
    "    tests:\n",
    "    >>> mult2(4)\n",
    "    8\n",
    "    >>> mult2(4.0)\n",
    "    8.0\n",
    "    \"\"\"\n",
    "\n",
    "    return n * 2\n",
    "\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd624d1-063f-47e2-a36c-588681538b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the map\n",
    "list(map(mult2, ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1961f-5637-4139-90ef-08f3056a4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the filter\n",
    "list(filter(lambda x: x % 2 == 1, ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38feb708-0a18-41ad-86a1-474f77e57968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reduce, not built-in in standard Python, but needs to be imported from functools\n",
    "reduce(add, ex, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227113ff-3c30-4326-8a45-885eb3563d12",
   "metadata": {},
   "source": [
    "Strangely enough, Python didn't have a product function similar to the sum function until Python 3.8. I believe it is because Guide van Rossum (Python's benevolent dictator) thought nobody would use it. Or perhaps he thought you could easily build on yourself, of course, not using the reduce function, as Guido doesn't like that one either. I, however, will use the reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1a840-8b7f-46a6-9795-e797087e82c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(numbers: list[int | float]) -> int | float:\n",
    "    \"\"\"\n",
    "    desc:Function that returns the product of a list of numbers\n",
    "    tests:\n",
    "    >>> product([1,2,3])\n",
    "    6\n",
    "    >>> product([100,1000,10_000,0])\n",
    "    0\n",
    "    >>> product([100])\n",
    "    100\n",
    "    >>> product([2, 2.5])\n",
    "    5.0\n",
    "    \"\"\"\n",
    "    return reduce(mul, numbers, 1)\n",
    "\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c4721-266f-4435-924c-158f31b7060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "product(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bced459-3790-42a7-bbdd-71576d941f25",
   "metadata": {},
   "source": [
    "\n",
    "As you can see from all three functions, we added another function: mult2, the somewhat silly function I quickly wrote. Then there is a construct you often see in data science and engineering: the anonymous $\\lambda$ (lambda) function. Finally, there are the functions add and mul, which are usually used as operators. \n",
    "\n",
    "In functional style Python we can even apply a function partially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df4981-c6fa-4d3b-b68f-8e4b30f2fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_rdd: RDD = bag_rdd.map(partial(add, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698bd77-99e7-430e-a2bc-effacdd20acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bag_rdd.collect()\n",
    "except Py4JJavaError:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d54269-55de-4023-9e3f-5688aa94fa67",
   "metadata": {},
   "source": [
    "What happend? We were trying to apply an operation not defined for certain types! If we read the stack trace, we get the following error: `TypeError: unsupported operand type(s) for +:` We are trying to add two types together, and add is not defined as an operation for one of the types.\n",
    "\n",
    "Our RDD has the following elements:\n",
    "1. 1\n",
    "2. 'two'\n",
    "3. 3.0\n",
    "4. ('four', 4)\n",
    "5. {'five', 5}\n",
    "\n",
    "For 3 of the 5 elements, we get a type error if we try our function add on them.\n",
    "\n",
    "#### Lazy evaluation\n",
    "Maybe you are a bit surprised that we only get an error after we call `collect` on the RDD. The reason is that the `map` function, which we call first, is only evaluated after we perform an action by calling `collect`. This is called lazy evaluation, also known as call by need. The expression is only evaluated when the value is needed. PySpark uses the lazy evaluation strategy. Python's regular evaluation strategy is eager evaluation, where a function is evaluated once encountered.\n",
    "\n",
    "if you would define function as:\n",
    "\n",
    "```\n",
    "def eager(a:Any, b:Any) -> bool | Any:\n",
    "    if a == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return b\n",
    "```\n",
    "and call it like so: `eager(0, 1/0)` in Python you will get a zero division error, because Python will have evaluated both arguments. In a lazy language like Haskell this would pass the compiler, if b is not called then Haskell won't evaluate b, neither will PySpark as its evaluation strategy is lazy as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc9684-c470-4243-9629-9d872a9a1751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safer_add(value: Any) -> int | float:\n",
    "    \"\"\"\n",
    "    desc:function that only adds if that operation is defined for the type\n",
    "    tests:\n",
    "    >>> safer_add(1)\n",
    "    2\n",
    "    >>> safer_add(1.0)\n",
    "    2.0\n",
    "    >>> safer_add({1: 'one'})\n",
    "    {1: 'one'}\n",
    "    \"\"\"\n",
    "    if isinstance(value, (int, float, complex)):\n",
    "        return value + 1\n",
    "    return value\n",
    "\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b112ccf-626f-4326-9e3b-ff47ff314b4d",
   "metadata": {},
   "source": [
    "This is not the only way we can implement a safer add. As an exercise you should write one that\n",
    "uses an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4302748-c39d-4ceb-b3f3-a71836357f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_rdd: RDD = sc.parallelize(bag)\n",
    "\n",
    "bag_rdd.map(safer_add).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c1ffa-d8d6-41f8-8aa6-aac390f5e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_rdd.filter(lambda e: isinstance(e, Number)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3d4f5-a359-4d21-a772-4046f725ec86",
   "metadata": {},
   "source": [
    "## Anonymous or $\\lambda$ functions\n",
    "In data science and engineering, you quite often see expressions like: `lambda e: isinstance(e, Number)`. This is known as the anonymous or $\\lambda$ (lambda) function. The expression exists in three parts:\n",
    "\n",
    "1. lambda: This is the anonymous part, equivalent to def is_number.\n",
    "2. e: is the argument to the $\\lambda$ function, derived from a collection.\n",
    "3. After the colon `:` is the body of the function inclus return.\n",
    "\n",
    "$\\lambda$ functions derive from functional programming, and there is only one use: not to have to write an actual function and create code bloat. \n",
    "\n",
    "Howeve, if writing $\\lambda$ functions confuses you or is not your style, then write proper functions. Though, I find $\\lambda$ functions are often clearer and more concise. Studies have shown that the time a developer spends reading code to writing code is at least a 10:1 ratio, so writing code that is concise and readable is important. You will see $\\lambda$ functions often! \n",
    "\n",
    "As a rule, you test and comment on all properly defined functions. With lambda functions, you do not need to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1137f-338a-4b19-bb32-92fd39c7c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnumber(e: Any) -> bool:\n",
    "    \"\"\"\n",
    "    desc: Function that checks if e is a number here defines as a float or an int\n",
    "    tests:\n",
    "    >>> isnumber('a')\n",
    "    False\n",
    "    >>> isnumber(1)\n",
    "    True\n",
    "    >>> isnumber(3.14)\n",
    "    True\n",
    "    >>> isnumber(True)\n",
    "    True\n",
    "    \"\"\"\n",
    "    return isinstance(e, Number)\n",
    "\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9804e-f38e-42d6-9255-fac8d9b5c859",
   "metadata": {},
   "source": [
    "#### False is 0\n",
    "You might be surprised that `isnumber(True)` passes the test. A boolean is, after all, not a number. That a boolean is regarded as a number in Python is due to the programming language C, which much of Python is built upon. They didn't incorporate a separate Boolean concept; instead, they said False=0, True=1. Thus, you will see Python code like:\n",
    "```\n",
    " num, denom = frac\n",
    "    if denom: # equivalent to if True\n",
    "        gcd = math.gcd(num, denom)\n",
    "        return num // gcd, denom // gcd\n",
    "    return None\n",
    "```\n",
    "Python even expanded that idea with truthy values, so instead of saying that the truth needs to be 1, in Python, as long as a number isn't 0, it will evaluate to True. This idea seems to me to be silly. Sure, 0/1 could False/True and we should be able to say $T\\ne F$, but I prefer not to say that -5 is True per se. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b11c77-baeb-405c-b9ab-9880d54c682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x: int = -5\n",
    "if x:\n",
    "    print(\"madness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87c58f-ccde-4d2a-b61d-ff4ec39d7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_rdd.filter(isnumber).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0952fa2d-38bc-496f-aa7d-996673253927",
   "metadata": {},
   "source": [
    "#### Reduce \n",
    "When using reduce, it is imperative that the function you add to reduce is defined for all elements of the recursive structure that the function will operate on. Our bag_rdd cannot be reduced. The following example can be reduced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b0918-9cb5-4ac9-afdb-8bff2ab02d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag2: list[Any] = [1, 2.3, 3, 87.65, 10_000_000]\n",
    "bag2_rdd: RDD = sc.parallelize(bag2)\n",
    "\n",
    "bag2_rdd.reduce(mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583db75e-49c4-4cc0-986e-f624db2cbcb6",
   "metadata": {},
   "source": [
    "#### `reduce` in a distributed world.\n",
    "There is an important limit to using reduce in a distributed world: I cannot do the following in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130eec1e-59dc-41cd-b8e7-44a5267de6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce(sub, bag2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e649dec-76f1-42b2-9d83-65157ff7b6d4",
   "metadata": {},
   "source": [
    "Doing the same in PySpark will return silly answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477c1b7-9ff2-4a51-b391-ede3cd54ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag2_rdd.reduce(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94093b8f-0827-4335-8350-d1307cb66a0c",
   "metadata": {},
   "source": [
    "The functions you add to reduce need to be both \n",
    "[commutative and associative](https://en.wikipedia.org/wiki/Commutative_property). `add`, `mul`, `min`, and `max` are both commutative and associative. Substraction and division are not. The reason behind this is simple: we distribute the workload over several nodes; therefore, we need to know that if we combine the partial results, it does not matter in what order they come in or what the grouping is. Obviously, PySparks reduce does not check that the functions have these properties. This would take some serious math; you would have to prove these properties for all functions you give as an argument to reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35095391-4ac0-4460-b477-f92587283ec4",
   "metadata": {},
   "source": [
    "#### DataFrame == RDD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a91c0-3318-4b2e-9a68-9c7877058817",
   "metadata": {},
   "outputs": [],
   "source": [
    "df: DataFrame = spark.createDataFrame([[1], [2], [6]], schema=[\"column\"])\n",
    "df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c984aea-96d0-4876-a6f9-da0c0b8d27b2",
   "metadata": {},
   "source": [
    "As you can see a DataFrame is very much also an RDD. You can think of each row as dictionary;\n",
    "the key is the column name, the value is the element in the row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb13433-b09c-4962-8b4d-cec035fbf581",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4158625-e2af-4f94-95e0-06be194d2436",
   "metadata": {},
   "source": [
    "#### RDD conclusion\n",
    "You should work with the DataFrame as the data structure; it has a more intuitive API, and when working with persistent data, it makes more sense to take the column approach. Only when you have one of the two use cases of an RDD should you use them.\n",
    "1. You have an unordered collection of Python objects that can be pickled.\n",
    "2. You have unordered key-value pairs, like a Python dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9541bc4-7e11-449b-8682-376206f2c397",
   "metadata": {},
   "source": [
    "## User Defined Function UDF\n",
    "There is another option: if you want to extend PySpark with your own Python code, you can use Pandas UDF (user-defined functions). You can use `pyspark.sql.functions.udf` with your own defined function as an argument. PySpark will promote your function to work on columns.\n",
    "\n",
    "There are a few things you should know about UDFs:\n",
    "\n",
    "1. You cannot use conditional expressions in a UDF. A conditional expression (a.k.a. ternary expression) takes the form of: x if condition else y\n",
    "2. You cannot use short-circuiting boolean expressions. if a != None and a.getSomething() is an example of short circuiting, if a is None, the right part of the expression will never be evaluated in Python. However, it will be in PySpark, and a NameError will be thrown, after which you cannot continue. \n",
    "3. The UDF cannot use keyword arguments on the calling side. You can use them in your definition, but you can enforce their use. You cannot write `def wht(*, arg1: int, arg2:str)`.\n",
    "\n",
    "Let us create a type that is not available in PySpark: the fraction.\n",
    "\n",
    "In modern Python, I think we have three options to create a fraction object:\n",
    "\n",
    "1. We create a class. The advantage of that is that we can group all methods together in that class. We can also use properties to ensure the denominator does not equal 0.\n",
    "2. We can use a NamedTuple to create the Fraction and write functions instead of methods.\n",
    "3. We can use a type alias. \n",
    "\n",
    "I would opt for the NamedTuple because we are writing PySpark scripts and not object-oriented applications. However, as we promote our Python function to PySpark, we need to ensure that we work with types that can be translated to PySpark types and vice versa, so a type alias will have to do.We need to be sure that the types that we use can be translated into PySpark types and vice versa, so a type alias will have to do.Â  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59054a2-f32b-4372-b4b6-6f584a2b5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraction = Tuple[int, int]\n",
    "\n",
    "fractions: list[tuple[int, int]] = [(x, y) for x in range(10) for y in range(1, 10)]\n",
    "\n",
    "frac_df: DataFrame = spark.createDataFrame(fractions, [\"numerator\", \"denominator\"])\n",
    "\n",
    "frac_df.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501efc1-b6d8-46ed-9df4-f440e426879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_simplify_fraction(frac: Fraction) -> Fraction | None:\n",
    "    \"\"\"\n",
    "    desc: Function to simplify fractions\n",
    "    tests:\n",
    "    >>> py_simplify_fraction((3,6))\n",
    "    (1, 2)\n",
    "    >>> py_simplify_fraction((2,5))\n",
    "    (2, 5)\n",
    "    >>> py_simplify_fraction((2,0))\n",
    "\n",
    "    \"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        gcd = math.gcd(num, denom)\n",
    "        return num // gcd, denom // gcd  # // -> floor division\n",
    "    return None\n",
    "\n",
    "\n",
    "def py_fraction_to_float(frac: Fraction) -> float | None:\n",
    "    \"\"\"\n",
    "    desc: Function to retrieve a float rounded to two decimals from a fraction\n",
    "    tests:\n",
    "    >>> py_fraction_to_float((1,2))\n",
    "    0.5\n",
    "    >>> py_fraction_to_float((2,3))\n",
    "    0.67\n",
    "    >>> py_fraction_to_float((2,0))\n",
    "    \"\"\"\n",
    "    num, denom = frac\n",
    "    try:\n",
    "        return round(num / denom, 2)\n",
    "    except ZeroDivisionError:\n",
    "        return None\n",
    "\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef951c-b5db-44b9-a68f-246323bd632a",
   "metadata": {},
   "source": [
    "#### The importance of typing\n",
    "\n",
    "I cannot stress enough the importance of using typing, both in Python and in PySpark scripts. Not only does typing give you instant commenting on code, this is important for others who read your code, but even for you. Often, you will have forgotten that it was you who wrote something. Suddenly, you find yourself wondering why you made the choices that you did. You need your own code to be well documented. Furthermore, typing prevents errors, especially if used in conjunction with a type checker such as [MyPy](https://www.mypy-lang.org/).\n",
    "\n",
    "Now let's use these functions on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab5159-c70d-49f8-83e3-a35daa7621fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a type alias\n",
    "SparkFrac: ArrayType = T.ArrayType(T.LongType())\n",
    "\n",
    "simplify_fraction: Callable = F.udf(py_simplify_fraction, SparkFrac)\n",
    "\n",
    "df: DataFrame = frac_df.select(F.array(F.col(\"numerator\"), F.col(\"denominator\")).alias(\"fraction\"))\n",
    "\n",
    "df: DataFrame = df.withColumn(\"simplified_fraction\", simplify_fraction(F.col(\"fraction\")))\n",
    "df.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbaeaa3-498a-40f5-b7c0-46b914941c8c",
   "metadata": {},
   "source": [
    "#### Typing again\n",
    "Typing in Python is sometimes a bit messy. If I ask for the type of `simplify_fraction`, then the return I will receive is `function`. However, if I type it as function `simplify_fraction: function` I will get an error... In Python, we type a function with its abstract base class, `Callable`. \n",
    "\n",
    "Typing in Python will remain awkward because, at its core, Python is a dynamic language, yet we need to use typing for the reasons I mentioned before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3658b8-cf35-4175-a2c9-5a706f91b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "issubclass(type(simplify_fraction), Callable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb17520-a597-4a49-83a8-b4ed9fde5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_to_float: Callable = F.udf(py_fraction_to_float, T.DoubleType())\n",
    "\n",
    "df: DataFrame = df.withColumn(\n",
    "    \"fraction_to_float\", fraction_to_float(F.col(\"fraction\")).alias(\"float\")\n",
    ")\n",
    "df.show(n=50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd6b77-2afc-479b-b9eb-d3542e3bc292",
   "metadata": {},
   "source": [
    "#### Using a udf decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c2386-3eb6-45db-b2e2-841192d6b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(T.DoubleType())\n",
    "def reverse_fraction_to_float(frac: Fraction) -> float | None:\n",
    "    \"\"\"\n",
    "    desc: reverse the float the numerator -> & denominator -> numerator\n",
    "    tests: No tests because of the decorator\n",
    "    \"\"\"\n",
    "    num, denom = frac\n",
    "    if num:\n",
    "        return py_fraction_to_float((denom, num))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a52a0a-7e8c-4566-aad3-96fff6d916e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df: DataFrame = df.withColumn(\n",
    "    \"reverse_fraction_to_float\",\n",
    "    reverse_fraction_to_float(F.col(\"fraction\")).alias(\"reverse\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec3478-1311-4168-b564-5f1ab33a9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(n=50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f04cd-ae57-4ad2-bafd-c8e7052891ff",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "1. The RDD allows for more flexibility compared to the DataFrame.\n",
    "2. The RDD is the most low-level and flexible way of running code in the distributed PySpark environment. You will need to be careful with your types to prevent type errors.\n",
    "3. The API for RDD is heavily inspired by Google's MapReduce. Apache Hadoop is an open-source variation of MapReduce.\n",
    "4. Using the `udf` function, we can promote Python functions to work with DataFrames. It is like mapping the Python function to the DataFrame.\n",
    "5. You have to be careful with your data design; the types of your function input and output must be comparable to PySpark types.\n",
    "6. If you use `udf` as a decorator, you cannot simply test your functions using doctest or assert (logically). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7633a-3bd0-4a30-a812-13c9418c8572",
   "metadata": {},
   "source": [
    "#### Remark\n",
    "In the book, there are a few exercises. Amongst them is one to write a temperature converter, exercise 8.3. Below is my version of this. Unfortunately, Jonathan Rioux's version is old Python, not the kind of Python I would promote. In this example we use a parguet file. [Parquet](https://en.wikipedia.org/wiki/Apache_Parquet) is a Hadoop column-based file format.\n",
    "\n",
    "Here I show you the code as I think it should be, in general I think it is better to write helper functions outside the body of a function, furthermore use pattern matching with the `match/case` statement as it is much clearer than a myriad of ifs. I recommend you compare the two code examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0207907-1f34-4037-8777-e18405fa4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am not commeting any further on these helpers I feel the name and the types are sufficient\n",
    "def celsius_to_fahrenheit(degree: float | int) -> float:\n",
    "    \"\"\"\n",
    "    >>> celsius_to_fahrenheit(30)\n",
    "    86.0\n",
    "    \"\"\"\n",
    "    return degree * 9 / 5 + 32\n",
    "\n",
    "\n",
    "def fahrenheit_to_celsius(degree: float | int) -> float:\n",
    "    \"\"\"\n",
    "    >>> fahrenheit_to_celsius(86)\n",
    "    30.0\n",
    "    \"\"\"\n",
    "    return (degree - 32) * 5 / 9\n",
    "\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e97500-f2c7-4ce7-8942-92c954be5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_to_temp_converter(\n",
    "    value: float | int, domain: str, image: str\n",
    ") -> float | int | None:\n",
    "    \"\"\"\n",
    "    desc: Function to convert temperatures from domain to image.\n",
    "    The excepted temperature scales are: (C)elsius, (F)ahrenheit, (K)elvin, and (R)ankine\n",
    "    tests\n",
    "    >>> temp_to_temp_converter(30, \"C\", \"F\")\n",
    "    86.0\n",
    "    >>> temp_to_temp_converter(30, \"C\", \"C\")\n",
    "    30\n",
    "    >>> temp_to_temp_converter(32, \"F\", \"C\")\n",
    "    0.0\n",
    "    >>> temp_to_temp_converter(86, \"F\", \"K\")\n",
    "    303.15\n",
    "    >>> temp_to_temp_converter(0, \"C\", \"R\")\n",
    "    491.67\n",
    "    >>> temp_to_temp_converter(30, \"C\", \"G\")\n",
    "    >>> temp_to_temp_converter(30, \"Q\", \"R\")\n",
    "    \"\"\"\n",
    "    match domain:\n",
    "        case \"C\":\n",
    "            match image:\n",
    "                case \"C\":\n",
    "                    return value\n",
    "                case \"F\":\n",
    "                    return celsius_to_fahrenheit(value)\n",
    "                case \"K\":\n",
    "                    return value + 273.15\n",
    "                case \"R\":\n",
    "                    return celsius_to_fahrenheit(value) + 459.67\n",
    "                case _:\n",
    "                    None\n",
    "        case \"F\":\n",
    "            match image:\n",
    "                case \"C\":\n",
    "                    return fahrenheit_to_celsius(value)\n",
    "                case \"F\":\n",
    "                    return value\n",
    "                case \"K\":\n",
    "                    return fahrenheit_to_celsius(value) + 273.15\n",
    "                case \"R\":\n",
    "                    return value + 459.67\n",
    "                case _:\n",
    "                    None\n",
    "        case \"K\":\n",
    "            match image:\n",
    "                case \"C\":\n",
    "                    return value - 273.15\n",
    "                case \"F\":\n",
    "                    return celsius_to_fahrenheit(value - 273.15)\n",
    "                case \"K\":\n",
    "                    return value\n",
    "                case \"R\":\n",
    "                    return value * 1.8\n",
    "                case _:\n",
    "                    return None\n",
    "        case \"R\":\n",
    "            match image:\n",
    "                case \"C\":\n",
    "                    return fahrenheit_to_celsius(value - 459.67)\n",
    "                case \"F\":\n",
    "                    return value - 459.67\n",
    "                case \"K\":\n",
    "                    return value / 1.8\n",
    "                case \"R\":\n",
    "                    return value\n",
    "                case _:\n",
    "                    None\n",
    "        case _:\n",
    "            None\n",
    "\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b9d7d-76b8-4a81-80fa-361eab0ca6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = \"./ProgrammingProjects/SparkTest/DataAnalysisWithPythonAndPySpark-Data-trunk/\"\n",
    "\n",
    "gsod: DataFrame = (\n",
    "    reduce(\n",
    "        lambda x, y: x.unionByName(y, allowMissingColumns=True),\n",
    "        [\n",
    "            spark.read.parquet(f\"{path}gsod_noaa/gsod{year}.parquet\")\n",
    "            for year in range(2010, 2021)\n",
    "        ],\n",
    "    )\n",
    "    .dropna(subset=[\"year\", \"mo\", \"da\", \"temp\"])\n",
    "    .where(F.col(\"temp\") != 9999.9)\n",
    "    .drop(\"date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006a1bc-5660-4b13-9629-af575a07cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod: DataFrame = gsod.select(\n",
    "    \"stn\",\n",
    "    \"year\",\n",
    "    \"mo\",\n",
    "    \"da\",\n",
    "    \"temp\",\n",
    ")\n",
    "conv: DataFrame = gsod.withColumn(\n",
    "    \"converted_temp\", temp_to_temp_converter(gsod.temp, \"F\", \"C\")\n",
    ").select(\"*\", F.expr(\"round(converted_temp, 1)\").alias(\"celsius\")).drop(\"converted_temp\")\n",
    "conv.show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8958a-84c1-4ac8-bdde-0075556be6f2",
   "metadata": {},
   "source": [
    "## Treating big data as a bunch of small data, a.k.a. using Pandas\n",
    "\n",
    "With Pandas we can start treating big data as nothing more than a bunch of small data, using Series UDF for column transformations. The simplest of the Pandas UDFs are the series UDFs. The series UDF is a one-dimensional Numpy ndarray. A numpy ndarray is actually an N-dimensional array, based upon the Fortran and C contiguous arrays. An instance of class ndarray consists of a contiguous one-dimensional segment of computer memory (owned by the array or by some other object) combined with an indexing scheme that maps N integers into the location of an item in the block. This construct makes the numpy ndarray very fast, much faster than a Python collection such as a list or tuple.\n",
    "\n",
    "The Pandas series object takes a Column object as input and returns a Column object as output. In PySpark, it is mostly used to make use of the Pandas library, but also of all those libraries that Pandas works excellent with. Libraries such as Numpy (obviously), scikit-learn, and statsmodel all operate seamlessly with Pandas and thus in PySpark.\n",
    "\n",
    "You can basically promote a Pandas (or friends) function to the distributed world of PySpark. If you are like me, then you straight away wonder how, after all, the numpy ndarray is that fast because it is contiguous in memory, ergo on one computer. This is solved by using [Apache Arrow](https://arrow.apache.org/) with its in-memory columnar data format.\n",
    "\n",
    "There are four Series UDFs we will look at:\n",
    "\n",
    "1. Series to Series\n",
    "2. Iterator of Series to Iterator of Series\n",
    "3. Iterator of Multiple Series to Iterator of Series\n",
    "4. Series to Scalar\n",
    "\n",
    "I will use some examples from Databricks to illuminate the UDF, then show how to apply them to real data. I will start of with a **Series to Series** example.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5f73b-2a96-4681-9eeb-23b04becbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function we want to promote\n",
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    desc: function that multiplies to columns together element wise\n",
    "    test:\n",
    "    >>> multiply_func(pd.Series([1, 2, 3]),pd.Series([1, 2, 3]))\n",
    "    0    1\n",
    "    1    4\n",
    "    2    9\n",
    "    dtype: int64\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# the promotion\n",
    "multiply: Callable = F.pandas_udf(multiply_func, returnType=T.LongType())\n",
    "\n",
    "col: Series = pd.Series([1, 2, 3])\n",
    "\n",
    "\n",
    "df: DataFrame = spark.createDataFrame(pd.DataFrame(col, columns=[\"col\"]))\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(\n",
    "    multiply(F.col(\"col\"), F.col(\"col\")).alias(\"element_wise_mutiplication\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ba9ae-822d-42aa-a3ce-fc9e169f965d",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "If you remember any linear algebra, this is a simple vector operation. \n",
    "\n",
    "$\\begin{matrix}1 \\\\ 2 \\\\ 3 \\end{matrix} \\times \\begin{matrix}1 \\\\ 2 \\\\ 3 \\end{matrix} = \\begin{matrix}1 \\\\ 4 \\\\ 9 \\end{matrix}$ \n",
    "\n",
    "In Python, we call this vectorization, replacing explicit loops with array expressions. In general, vectorized array operations will often be one or two (or more) orders of magnitude faster than their pure Python equivalents, with the biggest impact seen in any kind of numerical computation. Vectorized operations in NumPy use highly optimised C and Fortran functions, resulting in cleaner and faster Python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf2b03-1e16-4ab2-8a11-9b5072952c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same technique on actual data with gsod and a decorator\n",
    "@F.pandas_udf(\"double\")\n",
    "def fahrenheit_to_celsius(degrees: pd.Series) -> pd.Series:\n",
    "    '''converts degrees in Fahrenheit to Celsius using vectorization'''\n",
    "    return round((degrees - 32) * 5 / 9, 1)\n",
    "\n",
    "\n",
    "gsod: DataFrame = gsod.withColumn(\"temp_in_celsius\", fahrenheit_to_celsius(gsod[\"temp\"]))\n",
    "gsod.select(F.col(\"temp\"), F.col(\"temp_in_celsius\")).distinct().show(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf133e-f1c3-4d98-8e65-1686b2d62e8d",
   "metadata": {},
   "source": [
    "But what if I want to use `fahrenheit_to_celsius` on a Pandas DataFrame? I cannot use the decorated function; this elevates the function to be used on a Spark DataFrame. It seems it is not very `DRY` (don't repeat yourself) to write the same function without the decorator. Luckily, you do not need to. If you call `__dict__` on the function `fahrenheit_to_celsius`, then you will see there is func key word that returns the actual function. We can use `fahrenheit_to_celsius.func` to call the undecorated function for use on a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6cf7b-68bf-41bf-a148-bfa9a61fed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fahrenheit_to_celsius.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16626e5f-afaf-4317-b930-436e696bbaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod_local: PandasDataFrame = gsod.filter(\n",
    "    \"year='2018' and mo='08' and stn='710920'\"\n",
    ").toPandas()\n",
    "gsod_local.assign(temp_in_c_again = fahrenheit_to_celsius.func(gsod_local['temp'])).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3f297-39d0-4b6e-94f4-79d4315183e8",
   "metadata": {},
   "source": [
    "#### `functools.wraps`\n",
    "What is nice is that the PySpark people have made the decorated functions easily inspectable by using [`functools.wraps`](https://docs.python.org/3/library/functools.html)  when writing the decorator. The wraps function takes the meta data of the decorated function and promotes it to the decorator. An example of the metadata of the function is its documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab68d5-239f-4ff1-b462-c3ece6dfc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "fahrenheit_to_celsius.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6977a-49c9-4ad2-b40e-2fa75dfe7012",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(fahrenheit_to_celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214e767-247d-4950-b765-1ace15ddf3f9",
   "metadata": {},
   "source": [
    "#### Iterator\n",
    "\n",
    "Before we move on to the Iterator of Series to Iterator of Series UDF, we need to answer the question: What is an iterator? An iterator provides a way to access the elements of an aggregated object sequentially without exposing its underlying representation. New traversal operations should be defined for an aggregate object without changing its interface. This is worded a bit opaquely, but it isn't that complicated.\n",
    "\n",
    "When you define an iterator in any programming language, you have to answer two questions:\n",
    "\n",
    "1. How do I get to the next element in the collection? \n",
    "2. Is there a next element?\n",
    "\n",
    "In Python, you can make a class iterable by including a definition of [`__getitem__`](https://docs.python.org/3/reference/datamodel.html#object.__getitem__); this way, you implement the sequence class. You can implement the `__iter__` function in the iterable class and create an iterator class to perform the actual iteration. By doing so, you follow [the design pattern](https://en.wikipedia.org/wiki/Iterator_pattern#) strictly. The Pythonic way is to use a generator function, you write a generator by using the yield keyword in the body of the function. Yield kan suspend and pick up where it left. \n",
    "\n",
    "Using a generator, we can make a sentence iterable and then iterate over it using Python's built-in `next()` function. Python has no `hasNext()` function; it uses the StopIteration error, which is thrown by the yield key word if there are no more elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e2de0-f6e2-447e-aaca-64e115604cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sentence:\n",
    "    text: str\n",
    "    words: list[str] = field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        RE_WORD = re.compile(r\"\\w+\")\n",
    "        self.words = RE_WORD.findall(self.text)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Sentence({reprlib.repr(self.text)})\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        for word in self.words:\n",
    "            yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79c0f4-7bd3-406b-90d4-a7a2dd575e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence: Sentence = Sentence(\"Croc is peckish!\")\n",
    "it: Generator = iter(sentence)\n",
    "for word in sentence:\n",
    "    print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf602b0-bc32-49dd-9d05-8f4a546ab0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(\"long\")\n",
    "def plus_one(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for x in batch_iter:\n",
    "        yield x + 1\n",
    "\n",
    "col: PandasDataFrame = pd.Series([1, 2, 3])\n",
    "df: DataFrame = spark.createDataFrame(pd.DataFrame(col, columns=[\"col\"]))\n",
    "\n",
    "df.select(F.col(\"col\"), plus_one(F.col(\"col\")).alias(\"added_one\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62f214c-e9c5-43ea-8055-b2142e83dbd1",
   "metadata": {},
   "source": [
    "Now I can imagine that you question the usefulness of this example. This Pandas UDF is useful when the UDF execution requires initialising some state, for example, loading a machine learning model file to apply inference to every input batch. This is called an expensive cold start, meaning you will have to do some computationally expensive operation, for instance, loading that machine learning model, before we can start our manipulation.\n",
    "\n",
    "A simple example of using some initial state, is using a broadcast. Broadcast a read-only variable to the cluster, returning a Broadcast object for reading it in distributed functions. Such as our UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e4e18-a685-46f1-9ad2-42d07f4c7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bc: Broadcast = spark.sparkContext.broadcast(3)\n",
    "\n",
    "\n",
    "@F.pandas_udf(\"long\")\n",
    "def plus_y(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    y = y_bc.value  # all nodes in the cluster should have y_bc\n",
    "    try:\n",
    "        for x in batch_iter:\n",
    "            yield x + y\n",
    "    finally:\n",
    "        pass\n",
    "\n",
    "\n",
    "df.select(\n",
    "    F.col(\"col\"),\n",
    "    plus_one(F.col(\"col\")).alias(\"added_one\"),\n",
    "    plus_y(F.col(\"col\")).alias(\"added_y\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb4506-80c9-4751-9f22-55b6ca3b5b59",
   "metadata": {},
   "source": [
    "**Iterator of Multiple Series to Iterator of Series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a8641-c42a-43bb-8800-681ffd669948",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(\"long\")\n",
    "def multiply_two_cols(\n",
    "    iterator: Iterator[Tuple[pd.Series, pd.Series]]\n",
    ") -> Iterator[pd.Series]:\n",
    "    '''function that returns an element wise multiplied column'''\n",
    "    for a, b in iterator:\n",
    "        yield a * b\n",
    "\n",
    "\n",
    "df.select(\n",
    "    F.col(\"col\"),\n",
    "    plus_one(F.col(\"col\")).alias(\"added_one\"),\n",
    "    plus_y(F.col(\"col\")).alias(\"added_y\"),\n",
    "    multiply_two_cols(F.col(\"added_one\"), F.col(\"added_y\")).alias(\"added_one_times_added_y\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab3271-d46d-4072-a47a-139185234772",
   "metadata": {},
   "source": [
    "As you can see, we use the [yield](https://docs.python.org/3/reference/simple_stmts.html#grammar-token-python-grammar-yield_stmt) keyword, which yields a result every time we need it.\n",
    "\n",
    "The yield keyword returns a generator function. This is not the same as an iterator; a generator controls the iterating behaviour of a loop. This is an idea from Barbera Liskov. In Python, generators are efficient because they are lazy; they only yield when needed.\n",
    "\n",
    "Let's see this pattern in conjunction with some actual data and write a function I know you have written in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ada35-81f9-4445-a8d1-cdfbd7b8dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(\"date\")\n",
    "def create_date(\n",
    "    ymd: Iterator[Tuple[pd.Series, pd.Series, pd.Series]]\n",
    ") -> Iterator[pd.Series]:\n",
    "    \"\"\"create a data from a tuple containing year month and date\"\"\"\n",
    "    for year, month, day in ymd:\n",
    "        yield pd.to_datetime(pd.DataFrame(dict(year=year, month=month, day=day)))\n",
    "\n",
    "\n",
    "gsod.select(\n",
    "    \"year\", \"mo\", \"da\", create_date(gsod[\"year\"], gsod[\"mo\"], gsod[\"da\"]).alias(\"date\")\n",
    ").distinct().show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b4363-edf3-4f60-a600-4711815a0315",
   "metadata": {},
   "source": [
    "**Series-to-Scalar**    \n",
    "This is akin to a PySpark aggregate function. A Series-to-Scalar pandas UDF defines an aggregation from one or more pandas Series to a scalar value, where each pandas Series represents a Spark column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9906102-ee8f-489a-81fb-2a9b70b8b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "df: DataFrame = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\"id\", \"value\")\n",
    ")\n",
    "\n",
    "\n",
    "@F.pandas_udf(\"double\")\n",
    "def mean_udf(value: pd.Series) -> float:\n",
    "    \"\"\"returns for the column value\"\"\"\n",
    "    return value.mean()\n",
    "\n",
    "\n",
    "df.select(mean_udf(F.col(\"value\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8616f425-114c-45a1-8277-d13a86e380fc",
   "metadata": {},
   "source": [
    "#### Distributed Pandas?\n",
    "\n",
    "With this technique, we are free to use the wealth of functions in the Pandas API. However, there is a caveat: as PySpark is a distributed environment and Pandas is not, we might need to take the composition of batches into account with certain user-defined functions, e.g.,\n",
    "\n",
    "- Group aggregate UDFs\n",
    "- Group map UDFs\n",
    "\n",
    "These two UDFs are PySpark's solution to the split-apply-combine pattern:\n",
    "\n",
    "1. Split the batches of data into groups based on some criteria. Using `groupBy`.\n",
    "2. Apply a function to each group/batch independently. For instance, aggregate functions like sum, filter functions based upon some predicate, and transformations like plus_one are examples of applications.\n",
    "3. Combine the batches into a unified data set.\n",
    "\n",
    "Apache Arrow will default to batches of 10.000; this is a good size, but there might be circumstances (you run this code on an old Toshiba laptop) where you might want to reduce the batch size. This can be done by altering `spark.sql.execution.arrow.batchSize`.\n",
    "\n",
    "Finally, to make use of the computing power, you need to make sure each batch/group can be loaded into memory. If one of your batches cannot be placed in memory, you will get an out-of-memory exception.\n",
    "\n",
    "The Pandas Series-to-Scalar is split-apply-combine pattern applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ebc73-7046-47c0-95ef-dea6a4a4cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"id\").agg(mean_udf(df[\"value\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a33857-5800-4a9f-a890-0c43ca7c7ffd",
   "metadata": {},
   "source": [
    "#### group map UDF\n",
    "A group map UDF returns a `pd.DataFrame` which PySpark elevates to a PySpark DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ebc1a-857a-4ee1-aff6-c93a40163ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_temperature(temp_by_day: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    function that returns a normalization of temperature for a site.\n",
    "    If the temperature is constant for the whole period, defaults to 0.5\n",
    "    \"\"\"\n",
    "    temp = temp_by_day.temp\n",
    "    answer = temp_by_day[[\"stn\", \"year\", \"mo\", \"da\", \"temp\"]]\n",
    "    if temp.min() == temp.max():\n",
    "        return answer.assign(temp_norm=0.5) # pandas method: assigns a new column to a DataFrame.\n",
    "    return answer.assign( \n",
    "        temp_norm=round((temp - temp.min()) /(temp.max() - temp.min()),1) # the normalisation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd3430c-3aff-457c-a0af-0f96be686605",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod_map: DataFrame = gsod.groupBy(\"stn\", \"year\", \"mo\").applyInPandas(\n",
    "    scale_temperature,\n",
    "    schema=\"stn string, year string, mo string, da string, temp double, temp_norm double\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a3f58-fdac-4716-9498-e3443f5ee6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod_map.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5216a-16d9-490e-a839-96da3328daf5",
   "metadata": {},
   "source": [
    "As you can see, because we are applying this function in pandas we do not need to use `pandas_udf`, \n",
    "see the documentation on [`applyInPandas`](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.GroupedData.applyInPandas.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103d22e-d456-4361-8073-214aeae84782",
   "metadata": {},
   "source": [
    "#### Revisiting temp_to_temp_converter\n",
    "The `temp_to_temp_converter` I wrote above can be easily adapted to use a `pandas_udf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0fb41-a64a-4f43-b8f9-e9a9a027e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_to_c(degrees: pd.Series) -> pd.Series:\n",
    "    '''converts degrees in Fahrenheit to Celsius'''\n",
    "    return (degrees - 32) * 5 / 9\n",
    "\n",
    "def c_to_f(degrees: pd.Series) -> pd.Series:\n",
    "    '''converts degrees in Celsius to Fahrenheit'''\n",
    "    return degrees * 9 / 5 + 32\n",
    "\n",
    "# these are constants change the value and code will break\n",
    "rankine: Final =  459.67\n",
    "kelvin: Final = 273.15\n",
    "\n",
    "def temp_to_temp_converter(\n",
    "    value: pd.Series, domain: str, image: str\n",
    ") -> pd.Series:\n",
    "        match domain:\n",
    "            case \"C\":\n",
    "                match image:\n",
    "                    case \"C\":\n",
    "                        return value\n",
    "                    case \"F\":\n",
    "                        return c_to_f(value)\n",
    "                    case \"K\":\n",
    "                        return value + kelvin\n",
    "                    case \"R\": \n",
    "                        return c_to_f(value) + rankine\n",
    "                    case _: \n",
    "                        return value.apply(lambda _: None)\n",
    "            case \"F\":\n",
    "                match image:\n",
    "                    case \"C\": \n",
    "                        return f_to_c(value)\n",
    "                    case \"F\": \n",
    "                        return value \n",
    "                    case \"K\": \n",
    "                        return f_to_c(value) + kelvin\n",
    "                    case \"R\": \n",
    "                        return value + rankine\n",
    "                    case _:\n",
    "                        value.apply(lambda _: None) \n",
    "            case \"K\":  \n",
    "                match image:\n",
    "                    case \"C\":\n",
    "                        return value - kelvin\n",
    "                    case \"F\": \n",
    "                        return c_to_f(value - kelvin) \n",
    "                    case \"K\":\n",
    "                        return value \n",
    "                    case \"R\": \n",
    "                        return value * 1.8 \n",
    "                    case _: \n",
    "                        return value.apply(lambda _: None)\n",
    "            case \"R\": \n",
    "                match image: \n",
    "                    case \"C\": \n",
    "                        return f_to_c(value - rankine)\n",
    "                    case \"F\": \n",
    "                        return value - rankine \n",
    "                    case \"K\": \n",
    "                        return value / 1.8 \n",
    "                    case \"R\": \n",
    "                        return value \n",
    "                    case _: \n",
    "                        value.apply(lambda _: None)\n",
    "            case _:\n",
    "                value.apply(lambda _: None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb6797-85a7-4e16-91fa-e9226735e1b3",
   "metadata": {},
   "source": [
    "#### Code comment\n",
    "\n",
    "1. Like the previous code and unlike Rioux's code, I think the conversion functions are general functions and should be kept out of the larger functions. This prevents clutter in your code.\n",
    "2. This does not apply to $\\lambda$ functions. I use a lambda here because I assign a whole series of None values to a Pandas Series object.\n",
    "3. Python does not know constants as other programming languages do. For instance, in Java, you have the final key word: `public static final double kelvin = 273.15`. Python's typing library knows the type `Final`, which indicates a constant. If you use MyPy, you will get an error if the constant is reassigned somewhere. The interpeter will not throw an error and just make the change. I opt for leaving these constants out of the function; they are general constants. If I put them in the function `temp_to_temp_converter`, I will change the scope of these constants to just that function. I therefore cannot use these constants outside of my function, though I might want to. There are ways to prevent such random access in Python, but that is out of the scope of this notebook; therefore, I just wrote some comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35345e4a-211c-4b94-8c23-621190363120",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsod: DataFrame = gsod.withColumn(\"converted_temp\", temp_to_temp_converter(gsod[\"temp\"], \"F\", \"C\"))\n",
    "gsod.select(\n",
    "    F.col(\"temp\"), F.round(F.col(\"converted_temp\"), 1).alias(\"converted_temp\")\n",
    ").distinct().show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0da9df-2da0-4967-9f6a-e96a694b2916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
