{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5f9b5f-d57a-4560-90c8-7d2e98a1a67c",
   "metadata": {},
   "source": [
    "## The DataFrame\n",
    "\n",
    "The PySpark data-structure (a data organisation and storage format) that we will use the most is the DataFrame. In modern PySpark, the standard data-structure is the DataFrame. A DataFrame organises data into a 2-dimensional table of rows and columns, much like a spreadsheet, but with named columns and some other abilities. \n",
    "\n",
    "The PySpark DataFrame is built upon Spark's older data-structure, the Resilient Distributed Dataset (RDD). The idea behind Spark is that modern datasets are too large for a single computer. These datasets need to be distributed over several computers, perhaps over several locations (the ubiquitous cloud), hence distributed dataset. Of course, such distribution must be done in a fault-tolerant manner so that the dataset can be restored in case of some disturbance, ergo resilient.\n",
    "\n",
    "The DataFrame is built upon the RDD and is the data-structure to use when working with Python. The Pandas library also uses the DataFrame as its core data-structure. In fact, you can feed a Pandas DataFrame to a Spark DataFrame. Before you can create or read a DataFrame you will need to open a Spark session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22d125-f4a4-4679-9287-ebba05521898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell for imports\n",
    "import os\n",
    "from doctest import testmod\n",
    "from typing import NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26198036-8171-40a1-ac6c-b0fbfc78836b",
   "metadata": {},
   "source": [
    "The Spark session is your entry point to use the functionality that Spark offers. Accordingly, you will always see a Spark script using the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8fd65-7332-4bb9-a20a-b572a0e1e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark: SparkSession = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b3a39-1ee0-497c-b9e5-2158143ee0d9",
   "metadata": {},
   "source": [
    "Spark is great at giving you feedback.\n",
    "Perhaps a little too good. The line below sets the log level to log only errors, which is probably what you want when writing code. When running the code in production, you will probably want a different log level. The default log level is `Warn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d221cd-2da8-4473-864b-6d239494c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec9e39-c818-474d-863c-dadb7e91a515",
   "metadata": {},
   "source": [
    "#### Introduction into PySpark and Python\n",
    "\n",
    "Actually, you cannot read these notebooks without understanding Python, so this is not a true Python introduction. Python is a great programming language; it is easy to learn, easy to read, easy to write, very flexible in use, and has a great ecosystem of libraries you can use, such as PySpark. However, Python is also quite prone to mistakes. Mistakes that can come as a surprise, even to the most experienced programmer. The people behind Python are well aware of this and continue to improve the toolkit you have to write better code.\n",
    "\n",
    "For the kind of scripting that is done by your average data scientist or engineer, the basics of writing clean, safer, and understandable code is not that complicated. If you start writing larger object-oriented applications, you will need more steps, but for these type of notebooks, the following four steps will result in better code:\n",
    "\n",
    "1. Typing\n",
    "2. Naming\n",
    "3. Documenting\n",
    "4. Testing\n",
    "\n",
    "In these notebooks, we will follow these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf4ad85-a2ab-411b-8a14-e98e8b6bef53",
   "metadata": {},
   "source": [
    "## Typing\n",
    "Typing is a more complicated issue in Python than it is in a statically typed language like Java or Scala. The latter even has adopted Haskell's type inference, so you don't have to specify type. Instead of being a strict typer from the start, you probably should grow into it. For Python, these types are no more than hints; they are not checked at compilation time. Python will never be a static typed language; instead, it relies on external type checkers, such as MyPy. [MyPy](https://mypy.readthedocs.io/en/stable/) has a great intro to typing in Python, which is a good place to start. You could also read [PEP 484](https://peps.python.org/pep-0484/), where the case for typing was made by Guido van Rossum himself. As Python treats types as hints I suggest you use them as such; as a form of documentation.\n",
    "\n",
    "First we need to create a DataFrame to do manipulation on. You can create a DataFrame in several ways, the easiest probably being to add a list of rows. e.g., `groceries = [[\"courgette\", 1, 0.75], [\"lentils\", 1, 1.45], [\"toetje\", 2, 3.75]]`\n",
    "\n",
    "This example comes from the book (it is a good book. Just the coding practice is at times a bit iffy!). Code like this we should avoid; at least we should try to type this; however, we cannot type this code in such a manner that it would pass a type checker. A list can only have one type, which is a bit odd; we will come back to this in a later notebook. More important is that this code does not capture the essence of what we are trying to do. The inner elements are obviously different from the outer list, which is just a container. The inner elements are more complex; they have attributes and values. It is better to treat them as objects in themselves instead of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f160d1-edc6-4f9b-95f6-44fa2330723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item(NamedTuple):\n",
    "    name: str\n",
    "    quantity: tuple[int, str]\n",
    "    price: float\n",
    "\n",
    "\n",
    "groceries: list[Item] = [\n",
    "    Item(name=\"courgette\", quantity=(1, \"piece\"), price=0.75),\n",
    "    Item(name=\"lentiles\", quantity=(150, \"grams\"), price=1.45),\n",
    "    Item(name=\"desert\", quantity=(2, \"piece\"), price=4.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489ded6-874d-4eeb-b37a-8c79904e600a",
   "metadata": {},
   "source": [
    "In the code above, I created the same grocery list, but now I added an `Item` to the list. The `Item`\n",
    "looks like a class, but underneath the exterior is just a tuple. `NamedTuple`s have fields accessible by attribute lookup as well as being indexable, iterable, and have a nice `repr` method defined. All of this allows us to not only inspect the items on the grocery list thoroughly but also to direct the input on the list. An item must have a name, which is a string, a quantity, which is a tuple, and a price, which is a float. Writing code like this enables you to write much cleaner code and will prevent faults from using the wrong type; more on this shortly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2062c-b917-4de1-be17-4367f02d81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "repr(groceries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e1b85-f8bf-48a6-8373-54621e1525c1",
   "metadata": {},
   "source": [
    "This type of code is straight forward to work with. If we want to know the total we can write\n",
    "very legible code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ce8e0-e836-4c2e-9c7d-5c5ced9f6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "total: float = sum([item.price for item in groceries])\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6061944-ca26-49fe-8ad4-91dacd92a23e",
   "metadata": {},
   "source": [
    "Using code from the book I can get a total too, but far less clear. I will have to index\n",
    "into the inner list. The meaning of `item[2]` is not immidiatly clear, unlike `item.price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be9c23-638d-4189-81a3-5ab7c264b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_shoddy: list[tuple[str, int, float]] = [\n",
    "    (\"courgette\", 1, 0.75),\n",
    "    (\"lentils\", 1, 1.45),\n",
    "    (\"desert\", 2, 4.0),\n",
    "]\n",
    "total: float = sum([item[2] for item in groceries_shoddy])\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6429a9f0-9484-4c61-8a3d-be22b7f7206f",
   "metadata": {},
   "source": [
    "A final example of the superiority of using a named tuple is that I can easily add to the complexity of our item, without creating difficult to read code. What a quantity is can very depending on the item, for instance courgette you will likely buy per piece and lentils you will likely by per weight. Let's code it up using the shoddy coding practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c97c2-a185-4da9-8537-33c74cbaea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries_shoddy: list[tuple[str, tuple[int, str], float]] = [\n",
    "    (\"courgette\", (1, \"piece\"), 0.75),\n",
    "    (\"lentils\", (150, \"gram\"), 1.45),\n",
    "    (\"desert\", (2, \"piece\"), 4.0),\n",
    "]\n",
    "type(groceries_shoddy[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfda7c2-8803-46de-b45f-1a8a6157b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in groceries:\n",
    "    print(type(t.quantity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95ace8-48ba-4ed7-8653-c1ee826b44b4",
   "metadata": {},
   "source": [
    "Let's make a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49511c-6e3c-4556-bb2e-9721deee8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groceries: DataFrame = spark.createDataFrame(\n",
    "    groceries, [\"item\", \"quantity\", \"price\"]\n",
    ")\n",
    "df_groceries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171f27b-361e-415b-8483-f9af46cfb6af",
   "metadata": {},
   "source": [
    "#### Warning: PySpark does not coerce types!\n",
    "You cannot carelessly mix certain types in PySpark like you could in Python. Spark will throw an error if you assign `desert.price=4`. As background information, Spark does not automatically coerce an `int` into a `float`. Spark, and thus PySpark, is less flexible than Python, which will coerce these values. This is quite important to remember when you move from Python to PySpark. There will be more examples where Python and PySpark are not one-on-one. Using a `NamedTuple` has the benefit of typed attributes, directing the input. If a programmer follows your design, this fault should not occur, even if she does not know PySpark does not coerce types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60751d9-ed37-4d7b-85d4-833372735e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groceries.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90723f65-30a9-4d6c-88c1-9344b354d370",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "Spark SQL will be the API you will mostly use. The Spark SQL API introduced the [DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html) as a data abstraction. The DataFrame allows you to work with structured (e.g., tabular) data but also with semi-structured data, such as full-text documents or JSON. A DataFrame is column-oriented;  when we start manipulating our data, we do this per column and not per row. \n",
    "\n",
    "In a normal environment, you will not have to design data as we did with `Item`; instead, you will read data from a source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70b285-551a-496d-ab47-ef13e127300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = (\n",
    "    \"./ProgrammingProjects/SparkTest/DataAnalysisWithPythonAndPySpark-Data-trunk/gutenberg_books/\"\n",
    ")\n",
    "\n",
    "pride_and_pred: DataFrame = spark.read.text(path + \"1342-0.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c1d91-a028-4fb1-9420-35253e50e08c",
   "metadata": {},
   "source": [
    "## Quick inspection tools\n",
    "There are a few easy and useful inspection tools for any DataFrame:\n",
    "1. `printSchema` which prints the schema of our DataFrame\n",
    "2. `show` which shows us the first 20 rows of our DataFrame\n",
    "3. `count` which counts the rows.\n",
    "4. `rdd.countApprox` which approximates the number of rows in the DataFrame, useful when working with very large DFs.\n",
    "5. `columns` is an attribute of the DataFrame that gives you a list of columns.\n",
    "6. `dtypes` is another attribute that gives you the column name and type being used for that column in the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6efdc-4f66-40d0-a5a6-d0c5edc0fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pride_and_pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b073f-1991-42b1-b21a-0a47838a3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pride_and_pred.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d65aa2-0fd9-471d-ab8e-a5ccae191c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"the row count of pride_and pred is {pride_and_pred.count()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f811b28-c3c5-4c23-b6a0-ca60941da499",
   "metadata": {},
   "outputs": [],
   "source": [
    "pride_and_pred.rdd.countApprox(timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064710bf-5a82-4dff-a4f1-5f4ef884efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pride_and_pred.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612d060-e4c4-4c96-8442-11f430743bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pride_and_pred.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bacade-669d-4ca3-94b8-1bb8aca85ea8",
   "metadata": {},
   "source": [
    "## PySpark.sql API\n",
    "We can manipulate our dataframe with all kinds of functions and methods from the [PySpark SQL API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html#) \n",
    "\n",
    "Below, we will use three functions:\n",
    "1. The select function: `DataFrame.select(*cols: ColumnOrName) → DataFrame`\n",
    "2. The split function: `Pyspark.sql.functions.split(str: ColumnOrName, pattern: str, limit: int = - 1) → pyspark.sql.column.Column`\n",
    "3. The alias function: `Column.alias(*alias: str, **kwargs: Any) → pyspark.sql.column.Column`\n",
    "\n",
    "In the PySpark community, it is a convention to `import pyspark.sql.functions as F` and then call the function as `F.split`. It is good programming practice to follow the convention. Studies have shown that code is read ten times more than it is written; adhering to conventions increases the legibility of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b6cee-35cb-4fe1-9f77-cc148d2a5b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines: DataFrame = pride_and_pred.select(\n",
    "    F.split(pride_and_pred.value, \" \").alias(\"line\")\n",
    ")\n",
    "\n",
    "lines.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829b81a-1fd2-4af7-886c-02c342f401f2",
   "metadata": {},
   "source": [
    "We can use the alias we have created to further manipulate our data. We want to get rid of these\n",
    "lists; instead, we want a column that contains all the words in these lists. Enter the explode function:\n",
    "\n",
    "`pyspark.sql.functions.explode(col: ColumnOrName) → pyspark.sql.column.Column`\n",
    "\n",
    "Explode returns a new row for each element in the given array or map. In other words, for every string in our list (array) of strings, we get a new row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9f8ea-456b-44c7-9f15-e67948a4f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words: DataFrame = lines.select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab54848-40a6-42ba-adce-5b329c5e01f7",
   "metadata": {},
   "source": [
    "## selecting columns \n",
    "PySpark's select is very much equivalent to SQL's select. The function definition of select is:\n",
    "\n",
    "`DataFrame.select(*cols: ColumnOrName) → DataFrame` \n",
    "\n",
    "As arguments, we give a string, a column, or a list. Columns in PySpark can again be selected in all the ways we would expect in Python, and with the col `pyspark.sql.functions.col(col: str) → pyspark.sql.column.Column` function: \n",
    "1. `select(words.word)`\n",
    "2. `select(words['word'])`\n",
    "3. `select(F.col('word'))`\n",
    "\n",
    "You will see all three being used, and you can, but there are slight differences under the hood: \n",
    " - words.word uses the dot notation you know from objects. `words` being the object, `word` being the attribute, using dot notation uses the `__get_attribute__` special method. This is the most inflexible of the three. In Python, attribute names cannot have special characters, start with a number, or have spaces. Using this notation on a column that includes, for instance, a space will result in a `SyntaxError`.\n",
    " - `words['word']` uses the `__get_item__` special method. By implementing the `__get_item__` method, the DataFrame implemented the Sequence interface, which allows us to slice. This type of implicit interface implementation is called `DuckTyping`, a term you might come across.\n",
    " - `F.col('word')` is the Spark native way of selecting a column and returns an expression. An expression is a construct that can be evaluated to determine its value. This is an important distinction because PySpark returns something that can be evaluated, we can operate on before the DataFrame is assigned. \n",
    "\n",
    "You may think we may call a column with get: `words.get('word')` but we cannot. `get` is not defined for `Column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5948d6-b6e8-441b-b9e8-bef89dbb618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lower: DataFrame = words.select(F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_lower.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810ceec-f0ed-4ad9-beb5-6dab3ee28d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_clean: DataFrame = words_lower.select(\n",
    "    F.regexp_extract(F.col(\"word_lower\"), \"[a-z]+\", 0).alias(\"word\")\n",
    ")\n",
    "words_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ddd059-11bc-49a3-b237-d54d4cf9696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_no_null: DataFrame = words_clean.filter(F.col(\"word\") != \"\")\n",
    "words_no_null.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1822079-fdc4-475f-9603-7e6c2a757866",
   "metadata": {},
   "source": [
    "In a few easy steps we have changed the text into an analysable format.\n",
    "\n",
    "Say we want to know the number of words  which have 12 or more letters we can simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f027aa4-1236-4dd4-85f0-91780480761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_words = words_no_null.filter(F.length(F.col(\"word\")) > 11)\n",
    "big_words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f149e-b27d-40f3-8c01-782d3550671a",
   "metadata": {},
   "source": [
    "**Jane Austin uses a lot of big words!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664facf6-fae4-4709-9287-51afdaf0a5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = \"./Downloads\"\n",
    "\n",
    "broadcast_logs: DataFrame = spark.read.csv(\n",
    "    path=os.path.join(path, \"BroadcastLogs_2018_Q3_M8.CSV\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    timestampFormat=\"yyyy-MM-dd\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d62659-41f0-433a-88dc-20eb36b79a0d",
   "metadata": {},
   "source": [
    "Most likely, you will want to analyse some form of tabular data. For instance, a CSV file. Similar to reading text, Spark comes with a CSV reader.\n",
    "\n",
    "The CSV file is downloadable via the author's [GitHub page](https://github.com/jonesberg/DataAnalysisWithPythonAndPySpark-Data/tree/trunk/broadcast_logs).  .\n",
    "\n",
    "From the point of view of reading someone's code, I would suggest you always use keyword arguments to methods and functions; at least you know what the arguments are for, sort of anyway. Using keywords makes your code more readable. There is, however, another benefit: you get to know methods and functions better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538ccea-9e98-493c-85c3-0268e889eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_logs.select(\"BroadcastLogID\", \"LogServiceID\", \"LogDate\").show(\n",
    "    n=5, truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9b906-2b83-4077-8e27-5a615e404b0d",
   "metadata": {},
   "source": [
    "As I said previously, by using `col` we are returned an expression, an expression we can use straight\n",
    "away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64782b-e52e-4501-8e6c-cdb27a1b8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_logs.select(\n",
    "    F.col(\"Duration\"),\n",
    "    F.col(\"Duration\").substr(1, 2).cast(\"int\").alias(\"hours\"),\n",
    "    F.col(\"Duration\").substr(4, 2).cast(\"int\").alias(\"Minutes\"),\n",
    "    F.col(\"Duration\").substr(7, 2).cast(\"int\").alias(\"Seconds\"),\n",
    ").distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea0814-e18a-4302-8547-7e28521d7807",
   "metadata": {},
   "source": [
    "or use the column values in a calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46cdba-79d0-4bb7-9d35-dc063a31c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_logs.select(\n",
    "    F.col(\"Duration\"),\n",
    "    (\n",
    "        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 3600\n",
    "        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
    "        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\n",
    "    ).alias(\"DurationInSeconds\"),\n",
    ").distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f307ce-9726-494c-9e2f-3b14af6bec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_logs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b22cb-e380-4667-b689-0588e173cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(broadcast_logs.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b2e96-72ab-4c6a-b122-61d2887f9df4",
   "metadata": {},
   "source": [
    "Two quick checks give us quite some insight into the meta data of our DataFrame. \n",
    "We have 30 columns, and quite a few are simply IDs; let's drop those. There are two ways to do that: either we use the function `drop` or we can write something in Python, some code that saves us from having to spell out all the names of the columns we want to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a4e66-c576-4eb6-b5f2-44e5001658ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_logs = broadcast_logs.select(\n",
    "    *[x for x in broadcast_logs.columns if x[-2:] != \"ID\"]\n",
    ")\n",
    "broadcast_logs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d7381-67dc-47bd-85a6-c93b98e254bc",
   "metadata": {},
   "source": [
    "We have used an alias above, but what if we want the original DataFrame with an added column.\n",
    "\n",
    "We use `withColumn` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0e196-33b0-403d-a69b-a9ed9ce88bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_logs = broadcast_logs.withColumn(\n",
    "    \"DurationInSeconds\",\n",
    "    (\n",
    "        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 3600\n",
    "        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
    "        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\n",
    "    ),\n",
    ")\n",
    "extended_logs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a02d6-494d-47fa-93d6-36ad74a76a90",
   "metadata": {},
   "source": [
    "we can sort these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8520cd-ce65-419f-886c-2186f6ffd7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_logs.select(sorted(extended_logs.columns)).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc80f1b-96f4-46b2-bd78-d018ce809464",
   "metadata": {},
   "source": [
    "We can also use standard statistical functions on a DataFrame:\n",
    "1. `describe`\n",
    "2. `summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d5ba9-a099-47a9-83a6-70ac4bd8b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_logs.select(F.col(\"DurationInSeconds\")).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c46c40-aca1-4c37-84f4-1abe7a09f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_logs.select(F.col(\"DurationInSeconds\")).summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec7033-e97b-4687-863a-3ebc98130c0d",
   "metadata": {},
   "source": [
    "## Naming functions, classes and variables\n",
    "We have discussed typing; we should also discuss naming, as this is also an important part of proper coding. I start by stating that Python conventions are set out in [PEP 8](https://peps.python.org/pep-0008/). Of course, who can remember to use all those conventions? I cannot; I use a linter. [Ruff](https://github.com/astral-sh/ruff) to be precise, Ruff is written in Rust and is the fastest linter and code formatter out there for Python. You should use a linter too. \n",
    "\n",
    "As for naming methods, variables, etc., there are a few things to remember:\n",
    "\n",
    "1. Use nouns, verbs, and adjectives. Verbs should indicate an action like `calculate()` or `print()`. Nouns describe a return value of the function or describe the variable meaningfully, e.g., `name()`, `user`. Adjectives add specificity to a name, for instance, `total_price()`. Mixing nouns, verbs, and adjectives together builds descriptive names like `calculate_total_price()`.\n",
    "2. Avoid ambiguity; do not use generic terms such as `process` or `data`. Names with multiple meanings, e.g., check, file, object. Finally, do not use abbreviations unless they are widely known.\n",
    "3. If you want the function or attribute to be treated as private; use an underscore `_private`.\n",
    "\n",
    "Everyone who writes code has used the variable `x`. I am not an exception, but keeping in mind that code is read 10 times more than it is written, I try my best.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122c1c7-5f78-4678-a3fa-72a783d2e0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
